{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "15QfB7RAuXAc"
      },
      "source": [
        "# Language Modeling using Ngram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gucid6KNuXAe"
      },
      "source": [
        "In this Exercise, we are going to create a bigram language model and its variation. We will build one model for each of the following type and calculate their perplexity:\n",
        "\n",
        "- Unigram Model\n",
        "- Bigram Model\n",
        "- Bigram Model with Laplace smoothing\n",
        "- Bigram Model with Interpolation\n",
        "- Bigram Model with Kneser-ney Interpolation\n",
        "\n",
        "We will also use NLTK which is a natural language processing library for python to make our lives easier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRRrn78ZjL54"
      },
      "outputs": [],
      "source": [
        "# #download corpus\n",
        "!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
        "!unzip BEST2010.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeyvLSptdKXj"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/jajdlqnp5h0ywvo/tokenized_wiki_sample.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 707,
      "metadata": {
        "id": "GjJDeG03uXAf"
      },
      "outputs": [],
      "source": [
        "# First we import necessary library such as math, nltk, bigram, and collections.\n",
        "import math\n",
        "import nltk\n",
        "import io\n",
        "import random\n",
        "from random import shuffle\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "random.seed(999)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HugXBHNEuXAh"
      },
      "source": [
        "BEST2010 is a free Thai NLP dataset by NECTEC usually used as a standard benchmark for various NLP tasks including language modeling. It is separated into 4 domains including article, encyclopedia, news, and novel. The data is already tokenized using '|' as a separator.\n",
        "\n",
        "For example,\n",
        "\n",
        "ตาม|ที่|นางประนอม ทองจันทร์| |กับ| |ด.ช.กิตติพงษ์ แหลมผักแว่น| |และ| |ด.ญ.กาญจนา กรองแก้ว| |ป่วย|สงสัย|ติด|เชื้อ|ไข้|ขณะ|นี้|ยัง|ไม่|ดี|ขึ้น|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 708,
      "metadata": {
        "id": "iu-AJSZZuXAi"
      },
      "outputs": [],
      "source": [
        "total_word_count = 0\n",
        "best2010 = []\n",
        "with open(\"BEST2010/news.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        line = line.strip()[:-1]  # remove the trailing |\n",
        "        total_word_count += len(line.split(\"|\"))\n",
        "        best2010.append(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 709,
      "metadata": {
        "id": "3WfpGgbruXAj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sentences in BEST2010 news dataset :\t30969\n",
            "Total word counts in BEST2010 news dataset :\t1660190\n"
          ]
        }
      ],
      "source": [
        "# For simplicity, we assumes that each line is a sentence.\n",
        "print(f\"Total sentences in BEST2010 news dataset :\\t{len(best2010)}\")\n",
        "print(f\"Total word counts in BEST2010 news dataset :\\t{total_word_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JD9iXF1uXAm"
      },
      "source": [
        "We separate the input into 2 sets, train and test data with 70:30 ratio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 710,
      "metadata": {
        "id": "_WGcQq_juXAm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sentences in BEST2010 news training dataset :\t21678\n",
            "Total word counts in BEST2010 news training dataset :\t1042797\n"
          ]
        }
      ],
      "source": [
        "sentences = best2010\n",
        "# The data is separated to train and test set with 70:30 ratio.\n",
        "train = sentences[: int(len(sentences) * 0.7)]\n",
        "test = sentences[int(len(sentences) * 0.7) :]\n",
        "\n",
        "# Training data\n",
        "train_word_count = 0\n",
        "for line in train:\n",
        "    for word in line.split(\"|\"):\n",
        "        train_word_count += 1\n",
        "print(\"Total sentences in BEST2010 news training dataset :\\t\" + str(len(train)))\n",
        "print(\"Total word counts in BEST2010 news training dataset :\\t\" + str(train_word_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17x6tW-3ae7Z"
      },
      "source": [
        "Here we load the data from Wikipedia which is also already tokenized. It will be used for answering questions in MyCourseville.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 711,
      "metadata": {
        "id": "0fAl6dTg_9HG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "wiki_data = pd.read_csv(\"tokenized_wiki_sample.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1W5bm-hbQXa"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Before training any language models, the first step we always do is process the data into the format suited for the LM.\n",
        "\n",
        "For this exercise, we will use NLTK to help process our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 712,
      "metadata": {
        "id": "4OIqxJB7P29D"
      },
      "outputs": [],
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends, flatten\n",
        "from nltk.lm.vocabulary import Vocabulary\n",
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy0ZN2_0bzRr"
      },
      "source": [
        "We begin by \"tokenizing\" our training set. Note that the data is already tokenized so we can just split it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 713,
      "metadata": {
        "id": "WQM0PXnXbzCN"
      },
      "outputs": [],
      "source": [
        "tokenized_train = [\n",
        "    [\"<s>\"] + t.split(\"|\") + [\"</s>\"] for t in train\n",
        "]  # \"tokenize\" each sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM2ylNRNcrg9"
      },
      "source": [
        "Next we create a vocabulary with the `Vocabulary` class from NLTK. It accepts a list of tokens so we flatten our sentences into one long sentence first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 714,
      "metadata": {
        "id": "Tbp-VmkHcq4d"
      },
      "outputs": [],
      "source": [
        "flat_tokens = list(\n",
        "    flatten(tokenized_train)\n",
        ")  # join all sentences into one long sentence\n",
        "vocab = Vocabulary(\n",
        "    flat_tokens, unk_cutoff=3\n",
        ")  # Words with frequency **below** 3 (not exactly 3) will not be considered in our vocab and will be converted to <UNK>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 716,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True True True\n"
          ]
        }
      ],
      "source": [
        "print(\"<UNK>\" in vocab, \"<s>\" in vocab, \"</s>\" in vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFnBHe6ScAaV"
      },
      "source": [
        "Then we replace low frequency words and pad each sentence with \\<s\\> in the front and \\</s\\> in the back of each sentence.\n",
        "\n",
        "Now _each_ sentence is going to look something like this:\n",
        "\\[\"\\<s\\>\", \"hello\", \"my\", \"name\", \"is\", \"\\<UNK\\>\", \"\\</s\\>\" \\]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 715,
      "metadata": {
        "id": "9q6QakuibxqN"
      },
      "outputs": [],
      "source": [
        "tokenized_train = [\n",
        "    [token if token in vocab else \"<UNK>\" for token in sentence]\n",
        "    for sentence in tokenized_train\n",
        "]\n",
        "padded_tokenized_train = [\n",
        "    list(pad_both_ends(sentence, n=2)) for sentence in tokenized_train\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn6GxaSFeSpD"
      },
      "source": [
        "Finally, we do the same for the test set and the wiki dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 717,
      "metadata": {
        "id": "D4N6qKrPadIj"
      },
      "outputs": [],
      "source": [
        "tokenized_test = [t.split(\"|\") for t in test]\n",
        "tokenized_test = [\n",
        "    [token if token in vocab else \"<UNK>\" for token in sentence]\n",
        "    for sentence in tokenized_test\n",
        "]\n",
        "padded_tokenized_test = [\n",
        "    list(pad_both_ends(sentence, n=2)) for sentence in tokenized_test\n",
        "]\n",
        "\n",
        "tokenized_wiki_test = [t.split(\"|\") for t in wiki_data[\"tokenized\"].tolist()]\n",
        "tokenized_wiki_test = [\n",
        "    [token if token in vocab else \"<UNK>\" for token in sentence]\n",
        "    for sentence in tokenized_wiki_test\n",
        "]\n",
        "padded_tokenized_wiki_test = [\n",
        "    list(pad_both_ends(sentence, n=2)) for sentence in tokenized_wiki_test\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHtCMFMluXAo"
      },
      "source": [
        "# Unigram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V1WQTGzuXAp"
      },
      "source": [
        "In this section, we will demonstrate how to build a unigram language model <br>\n",
        "**Important note:** <br>\n",
        "**\\<s\\>** = sentence start symbol <br>\n",
        "**\\</s\\>** = sentence end symbol\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd7qOd7KAYWM"
      },
      "source": [
        "# VERY IMPORTANT:\n",
        "\n",
        "- In this notebook, we will _not_ default the unknown token probability to `1/len(vocab)` but instead will treat it as a normal word and let the model learn its probability so that we can compare our results to NLTK.\n",
        "- **Also make sure that the code in this notebook can be executed without any problem. If we find that you used NLTK to answer questions in MyCourseVille and did not finish the assignment, you will receive a grade of 0 for this assignment.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 718,
      "metadata": {
        "id": "CTV-i9kdse58"
      },
      "outputs": [],
      "source": [
        "class UnigramModel:\n",
        "    def __init__(self, data, vocab):\n",
        "        self.unigram_count = defaultdict(lambda: 0.0)\n",
        "        self.word_count = 0\n",
        "        self.vocab = vocab\n",
        "        for sentence in data:\n",
        "            for w in sentence:  # [(word1, ), (word2, ), (word3, )...]\n",
        "                w = w[0]\n",
        "                if w in self.vocab:\n",
        "                    self.unigram_count[w] += 1.0\n",
        "                else:\n",
        "                    self.unigram_count[\"<UNK>\"] += 1.0\n",
        "                self.word_count += 1\n",
        "\n",
        "    def __getitem__(self, w):\n",
        "        w = w[0]  # [(word1, ), (word2, ), (word3, )...]\n",
        "        if w in self.vocab:\n",
        "            return self.unigram_count[w] / (self.word_count)\n",
        "        else:\n",
        "            return self.unigram_count[\"<UNK>\"] / (self.word_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 719,
      "metadata": {
        "id": "FnWJJ8Hqs8Qs"
      },
      "outputs": [],
      "source": [
        "train_unigrams = [\n",
        "    list(ngrams(sent, n=1)) for sent in padded_tokenized_train\n",
        "]  # creating the unigrams by setting n=1\n",
        "model = UnigramModel(train_unigrams, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 720,
      "metadata": {
        "id": "6coGxSY7uXAt"
      },
      "outputs": [],
      "source": [
        "def getLnValue(x):\n",
        "    return math.log(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 721,
      "metadata": {
        "id": "cFy8yhZjuXAv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-3.991273499731109\n",
            "-3.991273499731109\n",
            "Problability of a sentence 1.408776035744038e-16\n"
          ]
        }
      ],
      "source": [
        "# problability of 'นายก'\n",
        "print(getLnValue(model[\"นายก\"]))\n",
        "\n",
        "# for example, problability of 'นายกรัฐมนตรี' which is an unknown word is equal to\n",
        "print(getLnValue(model[\"นายกรัฐมนตรี\"]))\n",
        "\n",
        "# problability of 'นายก' 'ได้' 'ให้' 'สัมภาษณ์' 'กับ' 'สื่อ'\n",
        "prob = (\n",
        "    getLnValue(model[\"นายก\"])\n",
        "    + getLnValue(model[\"ได้\"])\n",
        "    + getLnValue(model[\"ให้\"])\n",
        "    + getLnValue(model[\"สัมภาษณ์\"])\n",
        "    + getLnValue(model[\"กับ\"])\n",
        "    + getLnValue(model[\"สื่อ\"])\n",
        "    + getLnValue(model[\"</s>\"])\n",
        ")\n",
        "print(\"Problability of a sentence\", math.exp(prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8EfqnDsuXAw"
      },
      "source": [
        "# Perplexity\n",
        "\n",
        "In order to compare language model we need to calculate perplexity. In this task you should write a perplexity calculation code for the unigram model. The result perplexity should be around 406.89 and\n",
        "376.86 on train and test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZHQ-6tVuXAx"
      },
      "source": [
        "## TODO #1 Calculate perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 722,
      "metadata": {
        "id": "kh0DwzoouXAx"
      },
      "outputs": [],
      "source": [
        "def getLnValue(x):\n",
        "    return math.log(x)\n",
        "\n",
        "\n",
        "def calculate_sentence_ln_prob(sentence, model):\n",
        "    prob = 0\n",
        "    for word in sentence:\n",
        "        prob += getLnValue(model[word])\n",
        "    return prob\n",
        "\n",
        "\n",
        "def perplexity(test, model):\n",
        "    sum_ln_prob = 0\n",
        "    sum_len = 0\n",
        "    for sent in test:\n",
        "        sum_ln_prob += calculate_sentence_ln_prob(sent, model)\n",
        "        sum_len += len(sent)\n",
        "\n",
        "    pp = math.exp(-sum_ln_prob / sum_len)\n",
        "\n",
        "    return pp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 723,
      "metadata": {
        "id": "X-t_8mEzRxT-"
      },
      "outputs": [],
      "source": [
        "test_unigrams = [list(ngrams(sent, n=1)) for sent in padded_tokenized_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 724,
      "metadata": {
        "id": "PztVYprdtBja"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "406.8950820766048\n",
            "376.86063648570286\n"
          ]
        }
      ],
      "source": [
        "print(perplexity(train_unigrams, model))\n",
        "print(perplexity(test_unigrams, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHnBXtt3b-OY"
      },
      "source": [
        "## Q1 MCV\n",
        "\n",
        "Calculate the perplexity of the model on the wiki test set and answer in MyCourseVille\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 725,
      "metadata": {
        "id": "JRd6hF_WSBl_"
      },
      "outputs": [],
      "source": [
        "wiki_test_unigrams = [list(ngrams(sent, n=1)) for sent in padded_tokenized_wiki_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 726,
      "metadata": {
        "id": "I_LiSohADNLC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498.25056811239347\n"
          ]
        }
      ],
      "source": [
        "print(perplexity([list(flatten(wiki_test_unigrams))], model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK0gaMf0uXA2"
      },
      "source": [
        "# Bigram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmTkAY_QuXA3"
      },
      "source": [
        "Next, you will create a better language model than a unigram (which is not much to compare with). But first, it is very tedious to count every pair of words that occur in our corpus by ourselves. Lucky for us, nltk provides us a simple library which will simplify the process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 727,
      "metadata": {
        "id": "Lv6r2LJ1uXA4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is how nltk generate bigram.\n",
            "<s> I\n",
            "I always\n",
            "always search\n",
            "search google\n",
            "google for\n",
            "for an\n",
            "an answer\n",
            "answer .\n",
            ". </s>\n",
            "\n",
            "<s> and </s> are used as a start and end of sentence symbol. respectively.\n"
          ]
        }
      ],
      "source": [
        "# example of nltk usage for bigram\n",
        "sentence = \"I always search google for an answer .\"\n",
        "padded_sentence = list(pad_both_ends(sentence.split(), n=2))\n",
        "\n",
        "print(\"This is how nltk generate bigram.\")\n",
        "for w1, w2 in bigrams(padded_sentence):\n",
        "    print(w1, w2)\n",
        "print(\"\\n<s> and </s> are used as a start and end of sentence symbol. respectively.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R2T-6i9uXA6"
      },
      "source": [
        "Now, you should be able to implement a bigram model by yourself. Also, you must create a new perplexity calculation for bigram. The result perplexity should be around 50.21 and inf on train and test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aYkjzTzuXA7"
      },
      "source": [
        "## TODO #3 Write Bigram Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 728,
      "metadata": {
        "id": "l4s7oSmjkNuU"
      },
      "outputs": [],
      "source": [
        "class BigramModel:\n",
        "    def __init__(self, data, vocab):\n",
        "        self.unigram_count = defaultdict(lambda: 0.0)\n",
        "        self.bigram_count = defaultdict(lambda: 0.0)\n",
        "        self.vocab = vocab\n",
        "     \n",
        "        for sentence in data:\n",
        "            for w1, w2 in sentence:\n",
        "                self.bigram_count[(w1, w2)] += 1.0\n",
        "                self.unigram_count[w1] += 1.0\n",
        "\n",
        "            # account of the last word of each sentence\n",
        "            self.unigram_count[sentence[-1][-1]] += 1.0\n",
        "\n",
        "    def __getitem__(self, bigram):\n",
        "        w1, w2 = bigram\n",
        "\n",
        "        bigram_count = self.bigram_count[(w1, w2)]\n",
        "        unigram_count = self.unigram_count[w1]\n",
        "\n",
        "        # will use this if specified so\n",
        "        # if bigram_count == 0:\n",
        "        #     print(f\"Bigram {w1} {w2} not found. Use 1/vocab\")\n",
        "        # return 1.0 / (len(self.vocab))\n",
        "        \n",
        "        return bigram_count / unigram_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3_Cgu6guXA-"
      },
      "source": [
        "## TODO #4 Write Perplexity for Bigram Model\n",
        "\n",
        "Sum perplexity score at a sentence level, instead of word level\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 729,
      "metadata": {
        "id": "hICoAhZjAxo1"
      },
      "outputs": [],
      "source": [
        "def getLnValueBigram(x):\n",
        "    return math.log(x)\n",
        "\n",
        "\n",
        "def calculate_sentence_ln_prob(sentence, model):\n",
        "    prob = 0\n",
        "    for bigram in sentence:\n",
        "        prob += getLnValueBigram(model[bigram]) if model[bigram] != 0 else float('-inf')\n",
        "    return prob\n",
        "\n",
        "\n",
        "def perplexity(bigram_data, model):\n",
        "    sum_ln_prob = 0\n",
        "    sum_len = 0\n",
        "    for sent in bigram_data:\n",
        "\n",
        "        sum_ln_prob += calculate_sentence_ln_prob(sent, model)\n",
        "        sum_len += len(sent) \n",
        "\n",
        "    pp = math.exp(-sum_ln_prob / sum_len)\n",
        "\n",
        "    return pp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 730,
      "metadata": {
        "id": "NxJYI3_TS2gf"
      },
      "outputs": [],
      "source": [
        "train_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_train]\n",
        "test_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 731,
      "metadata": {
        "id": "A4DD_RPFtxUo"
      },
      "outputs": [],
      "source": [
        "bigram_model_scratch = BigramModel(train_bigrams, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 732,
      "metadata": {
        "id": "yw4BubpbtuV7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50.21343110065736\n",
            "24.977802535470772\n",
            "inf\n"
          ]
        }
      ],
      "source": [
        "print(perplexity([list(flatten(train_bigrams))], bigram_model_scratch))\n",
        "print(perplexity([list(flatten(test_bigrams))[:17]], bigram_model_scratch))\n",
        "print(perplexity([list(flatten(test_bigrams))], bigram_model_scratch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRv294uQcZFC"
      },
      "source": [
        "## Q2 MCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 733,
      "metadata": {
        "id": "kCeRCyOIUWTS"
      },
      "outputs": [],
      "source": [
        "wiki_test_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_wiki_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 734,
      "metadata": {
        "id": "q47hutRqIg1z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inf\n"
          ]
        }
      ],
      "source": [
        "print(perplexity([list(flatten(wiki_test_bigrams))], bigram_model_scratch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BAF9DQbuXBC"
      },
      "source": [
        "# Smoothing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlm75BWLuXBC"
      },
      "source": [
        "Usually any ngram models have a sparsity problem, which means it does not have every possible ngram of words in the dataset. Smoothing techniques can alleviate this problem. In this section, you will implement three basic smoothing methods laplace smoothing, interpolation for bigram, and Knesey-Ney smoothing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwa7YQiouXBD"
      },
      "source": [
        "## TODO #5 write Bigram with Laplace smoothing (Add-One Smoothing)\n",
        "\n",
        "The result perplexity on training and testing should be:\n",
        "\n",
        "    307.29, 364.17 for Laplace smoothing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 735,
      "metadata": {
        "id": "j2Bw4C9T_UEs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "307.2932191431376\n",
            "364.17463606907467\n"
          ]
        }
      ],
      "source": [
        "class BigramWithLaplaceSmoothing:\n",
        "    def __init__(self, data, vocab):\n",
        "        self.unigram_count = defaultdict(lambda: 0.0)\n",
        "        self.bigram_count = defaultdict(lambda: 0.0)\n",
        "        self.vocab = vocab\n",
        "\n",
        "        for sentence in data:\n",
        "            for w1, w2 in sentence:\n",
        "                self.bigram_count[(w1, w2)] += 1.0\n",
        "                self.unigram_count[w1] += 1.0\n",
        "\n",
        "            # account of the last word of each sentence\n",
        "            self.unigram_count[sentence[-1][-1]] += 1.0\n",
        "\n",
        "    def __getitem__(self, bigram):\n",
        "        w1, w2 = bigram\n",
        "\n",
        "        bigram_count = self.bigram_count[(w1, w2)]\n",
        "        unigram_count = self.unigram_count[w1]\n",
        "        \n",
        "        return (bigram_count+1) / (unigram_count + len(self.vocab))\n",
        "\n",
        "\n",
        "model = BigramWithLaplaceSmoothing(train_bigrams, vocab)\n",
        "print(perplexity([list(flatten(train_bigrams))], model))\n",
        "print(perplexity([list(flatten(test_bigrams))], model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFT4uhIGhP0c"
      },
      "source": [
        "## Q3 MCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 736,
      "metadata": {
        "id": "jSH60cshIpDy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "738.5456651453641\n"
          ]
        }
      ],
      "source": [
        "print(perplexity([list(flatten(wiki_test_bigrams))], model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JDswBSIuXBG"
      },
      "source": [
        "## TODO #6 Write Bigram with Interpolation\n",
        "\n",
        "Set the lambda value as 0.7 for bigram, 0.25 for unigram, and 0.05 for unknown word.\n",
        "\n",
        "The result perplexity on training and testing should be:\n",
        "\n",
        "    62.44, 103.99 for Interpolation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 737,
      "metadata": {
        "id": "PIeDBLarvZUT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62.44269181334268\n",
            "103.99017321534633\n"
          ]
        }
      ],
      "source": [
        "class BigramWithInterpolation:\n",
        "\n",
        "    def __init__(self, data, vocab, l=0.7):\n",
        "        self.unigram_count = defaultdict(lambda: 0.0)\n",
        "        self.bigram_count = defaultdict(lambda: 0.0)\n",
        "        self.total_word_count = 0\n",
        "        self.vocab = vocab\n",
        "        self.l = l  # l for lambda\n",
        "        for sentence in data:\n",
        "            for w1, w2 in sentence:\n",
        "                self.bigram_count[(w1, w2)] += 1.0\n",
        "                self.unigram_count[w1] += 1.0\n",
        "                self.total_word_count += 1\n",
        "\n",
        "            # account of the last word of each sentence\n",
        "            self.unigram_count[w2] += 1.0\n",
        "            self.total_word_count += 1\n",
        "\n",
        "    def __getitem__(self, bigram):\n",
        "        w1, w2 = bigram\n",
        "        unigram_prob = self.unigram_count[w2] / self.total_word_count\n",
        "        bigram_prob = self.bigram_count[(w1, w2)] / self.unigram_count[w1]\n",
        "\n",
        "        return 0.7 * bigram_prob + 0.25 * unigram_prob + 0.05 * (1 / len(self.vocab))\n",
        "\n",
        "\n",
        "model = BigramWithInterpolation(train_bigrams, vocab)\n",
        "print(perplexity([list(flatten(train_bigrams))], model))\n",
        "print(perplexity([list(flatten(test_bigrams))], model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-GlmJUIhN7s"
      },
      "source": [
        "## Q4 MCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 738,
      "metadata": {
        "id": "EilXywU-IuNU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "255.71779470477514\n"
          ]
        }
      ],
      "source": [
        "print(perplexity([list(flatten(wiki_test_bigrams))], model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUorP-EWuXBM"
      },
      "source": [
        "## Language modeling on multiple domains\n",
        "\n",
        "Sometimes, we do not have enough data to create a language model for a new domain. In that case, we can improvised by combining several models to improve result on the new domain.\n",
        "\n",
        "In this exercise you will try to merge two language models from news and article domains to create a language model for the encyclopedia domain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 739,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vocab(data):\n",
        "    tokenized_data = [\n",
        "        [\"<s>\"] + t.split(\"|\") + [\"</s>\"] for t in data\n",
        "    ]\n",
        "    flat_tokens = list(flatten(tokenized_data))\n",
        "    vocab = Vocabulary(flat_tokens, unk_cutoff=3)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def get_bigram_data(data, vocab):\n",
        "    tokenized_test = [t.split(\"|\") for t in data]\n",
        "    tokenized_test = [\n",
        "        [token if token in vocab else \"<UNK>\" for token in sentence]\n",
        "        for sentence in tokenized_test\n",
        "    ]\n",
        "    padded_tokenized_test = [\n",
        "        list(pad_both_ends(sentence, n=2)) for sentence in tokenized_test\n",
        "    ]\n",
        "    bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_test]\n",
        "\n",
        "    return bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 740,
      "metadata": {
        "id": "Jel9Hx69uXBN"
      },
      "outputs": [],
      "source": [
        "# create encyclopeida data (test data)\n",
        "encyclo_data = []\n",
        "with open(\"BEST2010/encyclopedia.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        encyclo_data.append(line.strip()[:-1])\n",
        "encyclopedia_bigrams = get_bigram_data(encyclo_data, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlla-S8YYRur"
      },
      "source": [
        "(news) First, you should try to calculate perplexity of your bigram with interpolation on encyclopedia data. The perplexity should be around 240.75\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 745,
      "metadata": {
        "id": "gkRm8W4UWyfc"
      },
      "outputs": [],
      "source": [
        "# model = BigramModel(train_bigrams, vocab) # inf\n",
        "# model = BigramWithLaplaceSmoothing(train_bigrams, vocab) # 729.697\n",
        "model = BigramWithInterpolation(train_bigrams, vocab) # 240.746"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 746,
      "metadata": {
        "id": "x0l91qLEuXBP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "240.74578402349226\n"
          ]
        }
      ],
      "source": [
        "# 1) news only on \"encyclopedia\"\n",
        "print(perplexity([list(flatten(encyclopedia_bigrams))], model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwV9j9U-uXBR"
      },
      "source": [
        "## TODO #7 - Langauge Modelling on Multiple Domains\n",
        "\n",
        "Combine news and article datasets to create another bigram model and evaluate it on the encyclopedia data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9skdgo8muXBO"
      },
      "source": [
        "(article) For your information, a bigram model with interpolation using article data to test on encyclopedia data has a perplexity of 218.57\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 749,
      "metadata": {
        "id": "LOA8fd53uXBU"
      },
      "outputs": [],
      "source": [
        "# 2) article only on \"encyclopedia\"\n",
        "best2010_article = []\n",
        "with open(\"BEST2010/article.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        best2010_article.append(line.strip()[:-1])\n",
        "\n",
        "combined_total_word_count = 0\n",
        "for line in best2010_article:\n",
        "    combined_total_word_count += len(line.split(\"|\"))\n",
        "\n",
        "article_vocab = get_vocab(best2010_article)\n",
        "article_bigrams = get_bigram_data(best2010_article, article_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 750,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create encyclopeida data (test data)\n",
        "encyclopedia_bigrams_article_vocab = get_bigram_data(encyclo_data, article_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 751,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BigramWithInterpolation(article_bigrams, article_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 753,
      "metadata": {
        "id": "7bLYcPvXYHkB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity of the bigram model using article data with interpolation smoothing on encyclopedia test data 218.57479345888848\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"Perplexity of the bigram model using article data with interpolation smoothing on encyclopedia test data\",\n",
        "    perplexity([list(flatten(encyclopedia_bigrams_article_vocab))], model),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 757,
      "metadata": {
        "id": "wBjmLhUcuXBS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity of the combined Bigram model with interpolation smoothing on encyclopedia test data 242.88025282580364\n"
          ]
        }
      ],
      "source": [
        "# 3) train on news + article, test on \"encyclopedia\"\n",
        "best2010_article_and_news = best2010_article.copy()\n",
        "with open(\"BEST2010/news.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        best2010_article_and_news.append(line.strip()[:-1])\n",
        "\n",
        "combined_vocab = get_vocab(best2010_article_and_news)\n",
        "combined_bigrams = get_bigram_data(best2010_article_and_news, combined_vocab)\n",
        "encyclopedia_bigrams_combined_vocab = get_bigram_data(encyclo_data, combined_vocab)\n",
        "\n",
        "combined_model = BigramWithInterpolation(combined_bigrams, combined_vocab)\n",
        "print(\n",
        "    \"Perplexity of the combined Bigram model with interpolation smoothing on encyclopedia test data\",\n",
        "    perplexity([list(flatten(encyclopedia_bigrams_combined_vocab))], combined_model),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNPEhD7WuXBV"
      },
      "source": [
        "## TODO #8 - Kneser-ney on \"News\"\n",
        "\n",
        "<!-- Reimplement equation 4.33 in SLP textbook (https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf) -->\n",
        "\n",
        "Implement Bigram Knerser-ney LM. The result perplexity should be around 58.18, 93.84 on train and test data. Be careful not to mix up vocab from the above section!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 758,
      "metadata": {
        "id": "Y_8xFf7tBqpc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58.18312117005813\n",
            "46.16427141273723\n",
            "88.87482261840823\n",
            "93.8399459324311\n"
          ]
        }
      ],
      "source": [
        "class BigramKneserNey:\n",
        "    def __init__(self, data, vocab, d=0.75):\n",
        "        self.unigram_count = defaultdict(lambda: 0.0)\n",
        "        self.bigram_count = defaultdict(lambda: 0.0)\n",
        "        self.start_with_unique_count = defaultdict(lambda : set())\n",
        "        self.end_with_unique_count = defaultdict(lambda: set())\n",
        "        self.vocab = vocab\n",
        "        self.d = d\n",
        "\n",
        "        for sentence in data:\n",
        "            for w1, w2 in sentence:\n",
        "                self.bigram_count[(w1, w2)] += 1.0\n",
        "                self.unigram_count[w1] += 1.0\n",
        "                self.start_with_unique_count[w1].add(w2)\n",
        "                self.end_with_unique_count[w2].add(w1)\n",
        "\n",
        "            # account of the last word of each sentence\n",
        "            self.unigram_count[w2] += 1.0\n",
        "        self.bigram_size = len(self.bigram_count) # __getitem__ will increase size of bigram_count e.g. unknown (w1, w2) bigram\n",
        "\n",
        "    def __getitem__(self, bigram):\n",
        "        w1, w2 = bigram\n",
        "        # Pkn(w2|w1) = x + lambda_w1*p_con\n",
        "        x = max(self.bigram_count[(w1, w2)] - self.d, 0) / self.unigram_count[w1]\n",
        "        lambda_w1 = (self.d / self.unigram_count[w1]) * len(self.start_with_unique_count[w1])\n",
        "        p_con = len(self.end_with_unique_count[w2]) / self.bigram_size\n",
        "\n",
        "        return x + lambda_w1 * p_con\n",
        "\n",
        "\n",
        "model = BigramKneserNey(train_bigrams, vocab)\n",
        "print(perplexity([list(flatten(train_bigrams))], model))\n",
        "print(perplexity([list(flatten(train_bigrams))[:1000]], model))\n",
        "print(perplexity([list(flatten(test_bigrams))[:1000]], model))\n",
        "print(perplexity([list(flatten(test_bigrams))], model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULDScRw-g8Yn"
      },
      "source": [
        "## Q5 MCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 759,
      "metadata": {
        "id": "eSZ1Pb9WvfWC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "268.6766593898691\n"
          ]
        }
      ],
      "source": [
        "print(perplexity([list(flatten(wiki_test_bigrams))], model))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
