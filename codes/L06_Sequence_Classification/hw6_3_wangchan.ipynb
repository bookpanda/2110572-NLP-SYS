{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:07:37.611549Z",
     "iopub.status.busy": "2025-02-11T17:07:37.611196Z",
     "iopub.status.idle": "2025-02-11T17:07:55.816510Z",
     "shell.execute_reply": "2025-02-11T17:07:55.815527Z",
     "shell.execute_reply.started": "2025-02-11T17:07:37.611501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.30.1 which is incompatible.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.30.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (0.13.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.10.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.10.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft==0.10.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft==0.10.0) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2025.1.31)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft==0.10.0) (2024.2.0)\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.14.0\n",
      "    Uninstalling peft-0.14.0:\n",
      "      Successfully uninstalled peft-0.14.0\n",
      "Successfully installed peft-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q  transformers==4.30.1 datasets evaluate thaixtransformers\n",
    "!pip install peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:08:27.332718Z",
     "iopub.status.busy": "2025-02-11T17:08:27.332298Z",
     "iopub.status.idle": "2025-02-11T17:08:28.885234Z",
     "shell.execute_reply": "2025-02-11T17:08:28.884317Z",
     "shell.execute_reply.started": "2025-02-11T17:08:27.332688Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-11 17:08:27--  https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.18, 2620:100:601f:18::a27d:912\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6 [following]\n",
      "--2025-02-11 17:08:27--  https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucd755b735ab178bf1caa9240762.dl.dropboxusercontent.com/cd/0/inline/Cj7jouAiyLz1lOCxz62FjnDIrnUosmOSjDw63Y3D-JSHsS2DIlvXq13G0YGqmChz575h4ZODT69e2HPvNdI0xOYeU6MYdpRDekm1OK6_6D6HAbpiOYGIdwYmsQlQSUPJrxg/file# [following]\n",
      "--2025-02-11 17:08:28--  https://ucd755b735ab178bf1caa9240762.dl.dropboxusercontent.com/cd/0/inline/Cj7jouAiyLz1lOCxz62FjnDIrnUosmOSjDw63Y3D-JSHsS2DIlvXq13G0YGqmChz575h4ZODT69e2HPvNdI0xOYeU6MYdpRDekm1OK6_6D6HAbpiOYGIdwYmsQlQSUPJrxg/file\n",
      "Resolving ucd755b735ab178bf1caa9240762.dl.dropboxusercontent.com (ucd755b735ab178bf1caa9240762.dl.dropboxusercontent.com)... 162.125.9.15, 2620:100:601f:15::a27d:90f\n",
      "Connecting to ucd755b735ab178bf1caa9240762.dl.dropboxusercontent.com (ucd755b735ab178bf1caa9240762.dl.dropboxusercontent.com)|162.125.9.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2518977 (2.4M) [text/plain]\n",
      "Saving to: ‘clean-phone-data-for-students.csv’\n",
      "\n",
      "clean-phone-data-fo 100%[===================>]   2.40M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-02-11 17:08:28 (26.4 MB/s) - ‘clean-phone-data-for-students.csv’ saved [2518977/2518977]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:10:23.519606Z",
     "iopub.status.busy": "2025-02-11T17:10:23.519176Z",
     "iopub.status.idle": "2025-02-11T17:10:23.526215Z",
     "shell.execute_reply": "2025-02-11T17:10:23.525590Z",
     "shell.execute_reply.started": "2025-02-11T17:10:23.519574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:09:11.932222Z",
     "iopub.status.busy": "2025-02-11T17:09:11.931938Z",
     "iopub.status.idle": "2025-02-11T17:09:12.313384Z",
     "shell.execute_reply": "2025-02-11T17:09:12.312531Z",
     "shell.execute_reply.started": "2025-02-11T17:09:11.932200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b15da3353ff4285906d5effcbda47c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:09.074387Z",
     "iopub.status.busy": "2025-02-11T17:40:09.074052Z",
     "iopub.status.idle": "2025-02-11T17:40:09.129197Z",
     "shell.execute_reply": "2025-02-11T17:40:09.128484Z",
     "shell.execute_reply.started": "2025-02-11T17:40:09.074348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('clean-phone-data-for-students.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:09.230821Z",
     "iopub.status.busy": "2025-02-11T17:40:09.230574Z",
     "iopub.status.idle": "2025-02-11T17:40:09.263472Z",
     "shell.execute_reply": "2025-02-11T17:40:09.262683Z",
     "shell.execute_reply.started": "2025-02-11T17:40:09.230800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n",
       "      <td>enquire</td>\n",
       "      <td>payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n",
       "      <td>report</td>\n",
       "      <td>suspend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n",
       "      <td>enquire</td>\n",
       "      <td>internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n",
       "      <td>report</td>\n",
       "      <td>phone_issues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Sentence Utterance   Action        Object\n",
       "0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n",
       "1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n",
       "2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n",
       "3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n",
       "4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>10377</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Utterance   Action   Object\n",
       "count               16175    16175    16175\n",
       "unique              13389       10       33\n",
       "top           บริการอื่นๆ  enquire  service\n",
       "freq                   97    10377     2525"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(data_df.head())\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:09.390609Z",
     "iopub.status.busy": "2025-02-11T17:40:09.390292Z",
     "iopub.status.idle": "2025-02-11T17:40:09.414924Z",
     "shell.execute_reply": "2025-02-11T17:40:09.414112Z",
     "shell.execute_reply.started": "2025-02-11T17:40:09.390584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raw_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              input raw_label\n",
       "count         16175     16175\n",
       "unique        13389        33\n",
       "top     บริการอื่นๆ   service\n",
       "freq             97      2525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
       "       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n",
       "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
       "       'information', 'lost_stolen', 'balance_minutes', 'idd',\n",
       "       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n",
       "       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n",
       "       'Loyalty_card'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = data_df[[\"Sentence Utterance\", \"Object\"]]\n",
    "data_df.columns = ['input', 'raw_label']\n",
    "display(data_df.describe())\n",
    "display(data_df.raw_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:09.538739Z",
     "iopub.status.busy": "2025-02-11T17:40:09.538508Z",
     "iopub.status.idle": "2025-02-11T17:40:09.567311Z",
     "shell.execute_reply": "2025-02-11T17:40:09.566676Z",
     "shell.execute_reply.started": "2025-02-11T17:40:09.538719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>clean_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>2528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              input clean_label\n",
       "count         16175       16175\n",
       "unique        13389          26\n",
       "top     บริการอื่นๆ     service\n",
       "freq             97        2528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
       "       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n",
       "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
       "       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n",
       "       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df['clean_label']=data_df['raw_label'].str.lower().copy()\n",
    "data_df.drop('raw_label', axis=1, inplace=True)\n",
    "display(data_df.describe())\n",
    "display(data_df.clean_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:09.693968Z",
     "iopub.status.busy": "2025-02-11T17:40:09.693757Z",
     "iopub.status.idle": "2025-02-11T17:40:09.716070Z",
     "shell.execute_reply": "2025-02-11T17:40:09.715337Z",
     "shell.execute_reply.started": "2025-02-11T17:40:09.693949Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>clean_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13389</td>\n",
       "      <td>13389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       input clean_label\n",
       "count                                  13389       13389\n",
       "unique                                 13389          26\n",
       "top     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ     service\n",
       "freq                                       1        2111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = data_df.drop_duplicates(\"input\", keep=\"first\")\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "display(data_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:09.824084Z",
     "iopub.status.busy": "2025-02-11T17:40:09.823861Z",
     "iopub.status.idle": "2025-02-11T17:40:09.829696Z",
     "shell.execute_reply": "2025-02-11T17:40:09.828863Z",
     "shell.execute_reply.started": "2025-02-11T17:40:09.824064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# remove class `contact` as it has only 4 sample (cannot do train 80 val 10 test 10)\n",
    "data_df = data_df[data_df.clean_label != 'contact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:11.919501Z",
     "iopub.status.busy": "2025-02-11T17:40:11.919167Z",
     "iopub.status.idle": "2025-02-11T17:40:11.933175Z",
     "shell.execute_reply": "2025-02-11T17:40:11.932377Z",
     "shell.execute_reply.started": "2025-02-11T17:40:11.919466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Mappings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'payment',\n",
       " 1: 'package',\n",
       " 2: 'suspend',\n",
       " 3: 'internet',\n",
       " 4: 'phone_issues',\n",
       " 5: 'service',\n",
       " 6: 'nontruemove',\n",
       " 7: 'balance',\n",
       " 8: 'detail',\n",
       " 9: 'bill',\n",
       " 10: 'credit',\n",
       " 11: 'promotion',\n",
       " 12: 'mobile_setting',\n",
       " 13: 'iservice',\n",
       " 14: 'roaming',\n",
       " 15: 'truemoney',\n",
       " 16: 'information',\n",
       " 17: 'lost_stolen',\n",
       " 18: 'balance_minutes',\n",
       " 19: 'idd',\n",
       " 20: 'garbage',\n",
       " 21: 'ringtone',\n",
       " 22: 'rate',\n",
       " 23: 'loyalty_card',\n",
       " 24: 'officer'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'payment': 0,\n",
       " 'package': 1,\n",
       " 'suspend': 2,\n",
       " 'internet': 3,\n",
       " 'phone_issues': 4,\n",
       " 'service': 5,\n",
       " 'nontruemove': 6,\n",
       " 'balance': 7,\n",
       " 'detail': 8,\n",
       " 'bill': 9,\n",
       " 'credit': 10,\n",
       " 'promotion': 11,\n",
       " 'mobile_setting': 12,\n",
       " 'iservice': 13,\n",
       " 'roaming': 14,\n",
       " 'truemoney': 15,\n",
       " 'information': 16,\n",
       " 'lost_stolen': 17,\n",
       " 'balance_minutes': 18,\n",
       " 'idd': 19,\n",
       " 'garbage': 20,\n",
       " 'ringtone': 21,\n",
       " 'rate': 22,\n",
       " 'loyalty_card': 23,\n",
       " 'officer': 24}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data_df.copy().to_numpy()\n",
    "\n",
    "unique_label = data_df.clean_label.unique()\n",
    "\n",
    "label_2_num_map = dict(zip(unique_label, range(len(unique_label))))\n",
    "num_2_label_map = dict(zip(range(len(unique_label)), unique_label))\n",
    "\n",
    "print(\"Create Mappings\")\n",
    "display(num_2_label_map)\n",
    "display(label_2_num_map)\n",
    "\n",
    "# print(\"Before Mappings\")\n",
    "# display(data[:, 1])\n",
    "data[:,1] = np.vectorize(label_2_num_map.get)(data[:,1])\n",
    "# print(\"After Mappings\")\n",
    "# display(data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:12.110483Z",
     "iopub.status.busy": "2025-02-11T17:40:12.110207Z",
     "iopub.status.idle": "2025-02-11T17:40:12.143070Z",
     "shell.execute_reply": "2025-02-11T17:40:12.142360Z",
     "shell.execute_reply.started": "2025-02-11T17:40:12.110426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def strip_str(string):\n",
    "    return string.strip()\n",
    "     \n",
    "# print(\"Before\")\n",
    "# print(data)\n",
    "data[:,0] = np.vectorize(strip_str)(data[:,0])\n",
    "# print(\"After\")\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 WangchanBERTa\n",
    "\n",
    "We ask you to train a WangchanBERTa-based model.\n",
    "\n",
    "We recommend you use the thaixtransformers fork (which we used in the PoS homework).\n",
    "https://github.com/PyThaiNLP/thaixtransformers\n",
    "\n",
    "The structure of the code will be very similar to the PoS homework. You will also find the huggingface [tutorial](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) useful. Or you can also add a softmax layer by yourself just like in the previous homework.\n",
    "\n",
    "Which WangchanBERTa model will you use? Why? (Don't forget to clean your text accordingly).\n",
    "\n",
    "**Ans:**\n",
    "`airesearch/wangchanberta-base-att-spm-uncased` because it is suitable for text classification. It was train on these datasets for Multiclass text classification:\n",
    "- `wisesight_sentiment`: 4-class text classification task (positive, neutral, negative, and question) based on social media posts and tweets\n",
    "- `wongnai_reivews`: Users' review rating classification task (scale is ranging from 1 to 5)\n",
    "- `generated_reviews_enth`: Generated users' review rating classification task (scale is ranging from 1 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:59:18.118086Z",
     "iopub.status.busy": "2025-02-11T17:59:18.117771Z",
     "iopub.status.idle": "2025-02-11T17:59:18.122708Z",
     "shell.execute_reply": "2025-02-11T17:59:18.121859Z",
     "shell.execute_reply.started": "2025-02-11T17:59:18.118065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline, DataCollatorWithPadding, AutoModel\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from thaixtransformers import Tokenizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:49:00.938124Z",
     "iopub.status.busy": "2025-02-11T17:49:00.937771Z",
     "iopub.status.idle": "2025-02-11T17:49:01.101305Z",
     "shell.execute_reply": "2025-02-11T17:49:01.100362Z",
     "shell.execute_reply.started": "2025-02-11T17:49:00.938096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
      "The class this function is called from is 'WangchanbertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "tokenizer = Tokenizer(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:18.606764Z",
     "iopub.status.busy": "2025-02-11T17:40:18.606194Z",
     "iopub.status.idle": "2025-02-11T17:40:18.613475Z",
     "shell.execute_reply": "2025-02-11T17:40:18.612523Z",
     "shell.execute_reply.started": "2025-02-11T17:40:18.606726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5, 10, 2856, 3, 265, 3, 265, 3, 1992, 8, 330, 28, 2753, 12, 8, 10, 3, 743, 2508, 3441, 8, 10, 3, 1276, 714, 8321, 110, 8, 3310, 2148, 14999, 8, 10, 1836, 4813, 11, 570, 8, 10, 177, 8, 9484, 2301, 2390, 5824, 204, 11545, 8, 10, 313, 2801, 11, 4573, 8, 10, 177, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(data[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:21.220547Z",
     "iopub.status.busy": "2025-02-11T17:40:21.220170Z",
     "iopub.status.idle": "2025-02-11T17:40:23.523880Z",
     "shell.execute_reply": "2025-02-11T17:40:23.522926Z",
     "shell.execute_reply.started": "2025-02-11T17:40:21.220517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13385it [00:02, 5829.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, sample in tqdm(enumerate(data)):\n",
    "    cleaned_sentence = sample[0].replace('ํา', \"ำ\") # only cleaning needed for WangchanBERTa\n",
    "    data[i, 0] = tokenizer(cleaned_sentence) # no padding as we will use DataCollatorWithPadding to pad within batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:24.385070Z",
     "iopub.status.busy": "2025-02-11T17:40:24.384788Z",
     "iopub.status.idle": "2025-02-11T17:40:24.390951Z",
     "shell.execute_reply": "2025-02-11T17:40:24.390012Z",
     "shell.execute_reply.started": "2025-02-11T17:40:24.385049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13385,), (13385,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[:, 0]\n",
    "y = data[:, 1]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:24.568933Z",
     "iopub.status.busy": "2025-02-11T17:40:24.568697Z",
     "iopub.status.idle": "2025-02-11T17:40:24.574189Z",
     "shell.execute_reply": "2025-02-11T17:40:24.573469Z",
     "shell.execute_reply.started": "2025-02-11T17:40:24.568913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5, 10, 2856, 3, 265, 3, 265, 3, 1992, 8, 330, 28, 2753, 12, 8, 10, 3, 743, 2508, 3441, 8, 10, 3, 1276, 714, 8321, 110, 8, 3310, 2148, 14999, 8, 10, 1836, 4813, 11, 570, 8, 10, 177, 8, 9484, 2301, 2390, 5824, 204, 11545, 8, 10, 313, 2801, 11, 4573, 8, 10, 177, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:25.809932Z",
     "iopub.status.busy": "2025-02-11T17:40:25.809647Z",
     "iopub.status.idle": "2025-02-11T17:40:25.833882Z",
     "shell.execute_reply": "2025-02-11T17:40:25.833094Z",
     "shell.execute_reply.started": "2025-02-11T17:40:25.809911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10708,) (10708,) (1338,) (1338,) (1339,) (1339,)\n"
     ]
    }
   ],
   "source": [
    "X_train: np.ndarray\n",
    "y_train: np.ndarray\n",
    "X_val: np.ndarray\n",
    "y_val: np.ndarray\n",
    "X_test: np.ndarray\n",
    "y_test: np.ndarray\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=0.2,random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:40:38.753236Z",
     "iopub.status.busy": "2025-02-11T17:40:38.752923Z",
     "iopub.status.idle": "2025-02-11T17:40:38.758751Z",
     "shell.execute_reply": "2025-02-11T17:40:38.757684Z",
     "shell.execute_reply.started": "2025-02-11T17:40:38.753214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CallCenterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(val) for key, val in self.encodings[idx].items()\n",
    "            if key in ['input_ids', 'attention_mask'] # take only input_ids and attention_mask fields\n",
    "        }\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:00:02.308472Z",
     "iopub.status.busy": "2025-02-11T18:00:02.308133Z",
     "iopub.status.idle": "2025-02-11T18:00:02.313469Z",
     "shell.execute_reply": "2025-02-11T18:00:02.312406Z",
     "shell.execute_reply.started": "2025-02-11T18:00:02.308447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CallCenterDataset(X_train, y_train)\n",
    "val_dataset = CallCenterDataset(X_val, y_val)\n",
    "test_dataset = CallCenterDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=data_collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:05:22.110755Z",
     "iopub.status.busy": "2025-02-11T18:05:22.110363Z",
     "iopub.status.idle": "2025-02-11T18:05:22.117144Z",
     "shell.execute_reply": "2025-02-11T18:05:22.116193Z",
     "shell.execute_reply.started": "2025-02-11T18:05:22.110726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   5,   10, 4811,    8,   10, 3240,    8,   10,  177, 7268, 4090,  879,\n",
       "           6])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:05:23.196708Z",
     "iopub.status.busy": "2025-02-11T18:05:23.196395Z",
     "iopub.status.idle": "2025-02-11T18:05:23.203198Z",
     "shell.execute_reply": "2025-02-11T18:05:23.202223Z",
     "shell.execute_reply.started": "2025-02-11T18:05:23.196686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:00:08.376798Z",
     "iopub.status.busy": "2025-02-11T18:00:08.376469Z",
     "iopub.status.idle": "2025-02-11T18:00:08.383044Z",
     "shell.execute_reply": "2025-02-11T18:00:08.382007Z",
     "shell.execute_reply.started": "2025-02-11T18:00:08.376775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseModel(LightningModule):\n",
    "    def __init__(\n",
    "          self,\n",
    "          model_name: str = 'airesearch/wangchanberta-base-att-spm-uncased',\n",
    "          learning_rate: float = 2e-5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def get_embeddings(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids, attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # [CLS] token is the first token in the sequence (index 0)\n",
    "        cls_embeddings = hidden_states[:, 0, :]  # [batch_size, hidden_size]\n",
    "        \n",
    "        return cls_embeddings\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.get_embeddings(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:06:25.742175Z",
     "iopub.status.busy": "2025-02-11T18:06:25.741874Z",
     "iopub.status.idle": "2025-02-11T18:06:25.753015Z",
     "shell.execute_reply": "2025-02-11T18:06:25.752116Z",
     "shell.execute_reply.started": "2025-02-11T18:06:25.742152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LMWithLinearClassfier(BaseModel):\n",
    "    def __init__(\n",
    "          self,\n",
    "          model_name: str = 'airesearch/wangchanberta-base-att-spm-uncased',\n",
    "          ckpt_path: str = None,\n",
    "          learning_rate: float = 2e-5,\n",
    "          freeze_encoder_weights: bool = False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_name,\n",
    "            learning_rate\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if ckpt_path:\n",
    "            checkpoint = torch.load(ckpt_path)\n",
    "            encoder_state_dict = {k.replace(\"encoder.\", \"\"): v for k, v in checkpoint[\"state_dict\"].items() if k.startswith(\"encoder.\")}\n",
    "            self.encoder.load_state_dict(encoder_state_dict)\n",
    "\n",
    "        self.linear_layer = nn.Linear(768, 25)\n",
    "\n",
    "        if freeze_encoder_weights:\n",
    "          self.freeze_weights(self.encoder)  # Freeze model\n",
    "\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=25)\n",
    "\n",
    "    def freeze_weights(self, model):\n",
    "        for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        cls_embeddings = self.get_embeddings(input_ids, attention_mask)\n",
    "        logits = self.linear_layer(cls_embeddings)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        acc = self.accuracy(logits, labels)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        acc = self.accuracy(logits, labels)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "\n",
    "        acc = self.accuracy(logits, labels)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:59:27.283891Z",
     "iopub.status.busy": "2025-02-11T18:59:27.283548Z",
     "iopub.status.idle": "2025-02-11T18:59:28.511453Z",
     "shell.execute_reply": "2025-02-11T18:59:28.510775Z",
     "shell.execute_reply.started": "2025-02-11T18:59:27.283857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = LMWithLinearClassfier(\n",
    "    'airesearch/wangchanberta-base-att-spm-uncased',\n",
    "    ckpt_path=None,\n",
    "    freeze_encoder_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:59:30.458573Z",
     "iopub.status.busy": "2025-02-11T18:59:30.458210Z",
     "iopub.status.idle": "2025-02-11T18:59:30.504866Z",
     "shell.execute_reply": "2025-02-11T18:59:30.504146Z",
     "shell.execute_reply.started": "2025-02-11T18:59:30.458525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_acc\",  # Metric to monitor\n",
    "    mode=\"max\",  # \"min\" for loss, \"max\" for accuracy\n",
    "    save_top_k=1,  # Save only the best model(s)\n",
    "    save_weights_only=True, # Saves only weights, not the entire model\n",
    "    dirpath=\"./checkpoints/\", # Path where the checkpoints will be saved\n",
    "    filename=\"best_pretrained_w_linear_model-{epoch}-{val_acc:.2f}\", # Customized name for the checkpoint\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "linear_trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator='auto',\n",
    "    callbacks=[checkpoint_callback], # Add the ModelCheckpoint callback\n",
    "    gradient_clip_val=1.0,\n",
    "    precision=16, # Mixed precision training\n",
    "    devices=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:59:34.253638Z",
     "iopub.status.busy": "2025-02-11T18:59:34.253290Z",
     "iopub.status.idle": "2025-02-11T19:03:04.460041Z",
     "shell.execute_reply": "2025-02-11T19:03:04.459239Z",
     "shell.execute_reply.started": "2025-02-11T18:59:34.253609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983b508e22d345c9bb3dd7093309c2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 210.2021267414093\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Train the model\n",
    "linear_trainer.fit(model, train_loader, val_loader)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T19:09:07.967475Z",
     "iopub.status.busy": "2025-02-11T19:09:07.967132Z",
     "iopub.status.idle": "2025-02-11T19:09:09.746040Z",
     "shell.execute_reply": "2025-02-11T19:09:09.745165Z",
     "shell.execute_reply.started": "2025-02-11T19:09:07.967418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c78ea3a0032423ea600438799872963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6818521022796631     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.19138503074646      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6818521022796631    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.19138503074646     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.7742555141448975\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result = linear_trainer.test(model, test_loader)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
