{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vdx6X6c8kJn"
   },
   "source": [
    "# HW3: Transformer from Scratch\n",
    "\n",
    "In this exercise, you are replicating character-level transformer from scratch with Pytorch Lightning. You should end up with similar code to [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
    "\n",
    "We have prepared for you a dataset and dataloader of พระอภัยมณี by สุนทรภู่ , a famous Thai poet. You should receive your very own nanoสุนทรภู่ by the end of this exercise.\n",
    "\n",
    "Reference: [Andrej Kaparty - Let's build GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY).  \n",
    "Data Source: [Vajirayana - พระอภัยมณี](https://vajirayana.org/%E0%B8%9E%E0%B8%A3%E0%B8%B0%E0%B8%AD%E0%B8%A0%E0%B8%B1%E0%B8%A2%E0%B8%A1%E0%B8%93%E0%B8%B5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:53:14.755389Z",
     "iopub.status.busy": "2025-01-22T15:53:14.755073Z",
     "iopub.status.idle": "2025-01-22T15:53:21.489462Z",
     "shell.execute_reply": "2025-01-22T15:53:21.488617Z",
     "shell.execute_reply.started": "2025-01-22T15:53:14.755361Z"
    },
    "id": "u544827zmHRK",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h--2025-01-22 15:53:20--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt [following]\n",
      "--2025-01-22 15:53:21--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3231076 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘pra-apai-manee-ch1-50.txt’\n",
      "\n",
      "pra-apai-manee-ch1- 100%[===================>]   3.08M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-01-22 15:53:21 (46.2 MB/s) - ‘pra-apai-manee-ch1-50.txt’ saved [3231076/3231076]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip -q install lightning\n",
    "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:53:21.491471Z",
     "iopub.status.busy": "2025-01-22T15:53:21.491117Z",
     "iopub.status.idle": "2025-01-22T15:53:30.683820Z",
     "shell.execute_reply": "2025-01-22T15:53:30.683027Z",
     "shell.execute_reply.started": "2025-01-22T15:53:21.491432Z"
    },
    "id": "hRBXmNByL5hP",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idhibhatpankam/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116159030>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.types import Tensor\n",
    "import lightning as L\n",
    "from datetime import datetime\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # B: how many independent sequences will we process in parallel?\n",
    "seq_len = 256    # T: what is the maximum context length for predictions?\n",
    "n_embd = 64     # C: text embedding size\n",
    "n_head = 4      # number of heads\n",
    "n_layer = 4     # number of blocks\n",
    "max_iters = 5000\n",
    "eval_interval = 250\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:53:30.686102Z",
     "iopub.status.busy": "2025-01-22T15:53:30.685648Z",
     "iopub.status.idle": "2025-01-22T15:53:30.704268Z",
     "shell.execute_reply": "2025-01-22T15:53:30.702802Z",
     "shell.execute_reply.started": "2025-01-22T15:53:30.686079Z"
    },
    "id": "JE7GsqOy7Dzg",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters:  1100605\n",
      "๏ แต่ปางหลังยังมีกรุงกษัตริย์\n",
      "สมมุติวงศ์ทรงนามท้าวสุทัศน์\tผ่านสมบัติรัตนานามธานี\n",
      "อันกรุงไกรใหญ่ยาวสิบเก้าโยชน์\tภูเขาโขดเป็นกำแพงบุรีศรี\n",
      "สะพรึบพร้อมไพร่ฟ้าประชาชี\tชาวบุรีหรรษาสถาวร\n",
      "มีเอกองค์นงลักษณ์อัครราช\tพระนางนาฏนามปทุมเกสร\n",
      "สนมนางแสนสุรางคนิกร\tดังกินนรน่ารักลักขณา\n",
      "มีโอรสสององค์ล้วนทรงลักษณ์\tประไพพักตร์เพียงเทพเลขา\n",
      "ชื่ออภัยมณีเป็นพี่ยา\tพึ่งแรกรุ่นชันษาสิบห้าปี\n",
      "อันกุมารศรีสุวรรณนั้นเป็นน้อง\tเนื้อดังทองนพคุณจำรูญศรี\n",
      "พึ่งโสกันต์ชันษาสิบสามปี\tพระชนนีรักใคร่ดังนัยนา\n",
      "สมเด็จท้าวบิตุรงค์ดำรงราชย์\tแสนสวาทลูกน้อยเสน่หา\n",
      "จะเสกสองครองสมบัติขัตติยา\tแต่วิชาสิ่งใดไม่ชำนาญ\n",
      "จึงดำรัสเรียกพระโอรสราช\tมาริมอาสน์แท่นสุวรรณแล้วบรรหาร\n",
      "พ่อจะแจ้งเจ้าจงจำคำโบราณ\tอันชายชาญเชื้อกษัตริย์ขัตติยา\n",
      "ย่อมพากเพียรเรียนไสยศาสตร์เวท\tสิ่งวิเศษสืบเสาะแสวงหา\n",
      "ได้ป้องกันอันตรายนครา\tตามกษัตริย์ขัตติยาอย่างโบราณ\n",
      "พระลูกรักจักสืบวงศ์กษัตริย์\tจงรีบรัดเสาะแสวงแห่งสถาน\n",
      "หาทิศาปาโมกข์ชำนาญชาญ\tเป็นอาจารย์พากเพียรเรียนวิชา ฯ\n",
      "๏ บัดนั้นพี่น้องสองกษัตริย์\tประนมหัตถ์อภิวันท์ด้วยหรรษา\n",
      "จึงทูลความตามจิตเจตนา\tลูกคิดมาจะประมาณก็นานครัน\n",
      "หวังแสวงไปตำ\n"
     ]
    }
   ],
   "source": [
    "with open('pra-apai-manee-ch1-50.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Length of dataset in characters: \", len(text))\n",
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:53:30.705525Z",
     "iopub.status.busy": "2025-01-22T15:53:30.705290Z",
     "iopub.status.idle": "2025-01-22T15:53:30.778485Z",
     "shell.execute_reply": "2025-01-22T15:53:30.777556Z",
     "shell.execute_reply.started": "2025-01-22T15:53:30.705493Z"
    },
    "id": "x2wvWIFaqbzb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Characters: \t\n",
      " กขคฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮฯะัาำิีึืุูเแโใไๅ็่้๊๋์๏\n",
      "Vocab Size: 71\n",
      "[42, 39, 49, 42, 20, 53, 5, 35, 49, 26]\n",
      "สวัสดีครับ\n"
     ]
    }
   ],
   "source": [
    "# Quick implementation of character tokenizer\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"All Characters: {''.join(chars)}\")\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"สวัสดีครับ\"))\n",
    "print(decode(encode(\"สวัสดีครับ\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:53:30.779505Z",
     "iopub.status.busy": "2025-01-22T15:53:30.779241Z",
     "iopub.status.idle": "2025-01-22T15:53:31.033065Z",
     "shell.execute_reply": "2025-01-22T15:53:31.032310Z",
     "shell.execute_reply.started": "2025-01-22T15:53:30.779484Z"
    },
    "id": "GWBxs6HNuYun",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1100605]) torch.int64\n",
      "(tensor([70,  2, 59, 21, 65, 27, 50,  7, 43, 37, 49,  7, 34, 49,  7, 33, 53,  3,\n",
      "        35, 56,  7,  3, 41, 49, 21, 35, 52, 34, 69,  1, 42, 33, 33, 56, 21, 52,\n",
      "        39,  7, 40, 69, 23, 35,  7, 25, 50, 33, 23, 66, 50, 39, 42, 56, 23, 49,\n",
      "        40, 25, 69,  0, 28, 65, 50, 25, 42, 33, 26, 49, 21, 52, 35, 49, 21, 25,\n",
      "        50, 25, 50, 33, 24, 50, 25, 53,  1, 45, 49, 25,  3, 35, 56,  7, 62,  3,\n",
      "        35, 61, 43, 13, 65, 34, 50, 39, 42, 52, 26, 58,  3, 66, 50, 60, 34, 10,\n",
      "        25, 69,  0, 32, 57, 58,  4, 50, 60,  4, 20, 58, 27, 64, 25,  3, 51, 59,\n",
      "        30,  7, 26, 56, 35, 53, 40, 35, 53,  1, 42, 48, 30, 35, 54, 26, 30, 35,\n",
      "        66, 45, 33, 62, 30, 35, 65, 31, 66, 50, 27, 35, 48, 10, 50, 10, 53,  0,\n",
      "        10, 50, 39, 26, 56, 35, 53, 43, 35, 35, 41, 50, 42, 22, 50, 39, 35,  1,\n",
      "        33, 53, 58, 45,  3, 45,  7,  5, 69, 25,  7, 37, 49,  3, 41, 19, 69, 45,\n",
      "        49,  5, 35, 35, 50, 10,  0, 30, 35, 48, 25, 50,  7, 25, 50, 15, 25, 50,\n",
      "        33, 27, 23, 56, 33, 58,  3, 42, 35,  1, 42, 25, 33, 25, 50,  7, 59, 42,\n",
      "        25, 42, 56, 35, 50,  7,  5, 25, 52,  3, 35,  0, 20, 49,  7,  3, 52, 25,\n",
      "        25, 35, 25, 65]), tensor([ 2, 59, 21, 65, 27, 50,  7, 43, 37, 49,  7, 34, 49,  7, 33, 53,  3, 35,\n",
      "        56,  7,  3, 41, 49, 21, 35, 52, 34, 69,  1, 42, 33, 33, 56, 21, 52, 39,\n",
      "         7, 40, 69, 23, 35,  7, 25, 50, 33, 23, 66, 50, 39, 42, 56, 23, 49, 40,\n",
      "        25, 69,  0, 28, 65, 50, 25, 42, 33, 26, 49, 21, 52, 35, 49, 21, 25, 50,\n",
      "        25, 50, 33, 24, 50, 25, 53,  1, 45, 49, 25,  3, 35, 56,  7, 62,  3, 35,\n",
      "        61, 43, 13, 65, 34, 50, 39, 42, 52, 26, 58,  3, 66, 50, 60, 34, 10, 25,\n",
      "        69,  0, 32, 57, 58,  4, 50, 60,  4, 20, 58, 27, 64, 25,  3, 51, 59, 30,\n",
      "         7, 26, 56, 35, 53, 40, 35, 53,  1, 42, 48, 30, 35, 54, 26, 30, 35, 66,\n",
      "        45, 33, 62, 30, 35, 65, 31, 66, 50, 27, 35, 48, 10, 50, 10, 53,  0, 10,\n",
      "        50, 39, 26, 56, 35, 53, 43, 35, 35, 41, 50, 42, 22, 50, 39, 35,  1, 33,\n",
      "        53, 58, 45,  3, 45,  7,  5, 69, 25,  7, 37, 49,  3, 41, 19, 69, 45, 49,\n",
      "         5, 35, 35, 50, 10,  0, 30, 35, 48, 25, 50,  7, 25, 50, 15, 25, 50, 33,\n",
      "        27, 23, 56, 33, 58,  3, 42, 35,  1, 42, 25, 33, 25, 50,  7, 59, 42, 25,\n",
      "        42, 56, 35, 50,  7,  5, 25, 52,  3, 35,  0, 20, 49,  7,  3, 52, 25, 25,\n",
      "        35, 25, 65, 50]))\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, data, seq_len):\n",
    "    self.data = data\n",
    "    self.seq_len = seq_len\n",
    "  def __len__(self):\n",
    "    return len(self.data)-seq_len\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx:idx+seq_len], self.data[idx+1:idx+seq_len+1]\n",
    "\n",
    "train_dataset = TextDataset(train_data, seq_len)\n",
    "val_dataset = TextDataset(val_data, seq_len)\n",
    "print(train_dataset[0])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qjjq6aoIV-J4"
   },
   "source": [
    "## Part 1: Self-Attention Head (Scaled Dot-Product Attention)\n",
    "\n",
    "This part implements the 3.2.1 Scaled Dot-Product Attention in the paper _Attention is All You Need_.\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} )V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:53:31.174207Z",
     "iopub.status.busy": "2025-01-22T15:53:31.173859Z",
     "iopub.status.idle": "2025-01-22T15:53:31.178507Z",
     "shell.execute_reply": "2025-01-22T15:53:31.177453Z",
     "shell.execute_reply.started": "2025-01-22T15:53:31.174181Z"
    },
    "id": "X05dtVO7l8VZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "B, T, C = batch_size, seq_len, n_embd # batch, time, channels\n",
    "head_size = n_embd//n_head    # 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtC0yuI0idPM"
   },
   "source": [
    "### 1.1 Implementing Queries, Keys, Values\n",
    "\n",
    "This should be the easiest step of the self-attention. Given $x$ with the shape of $B \\times T \\times C$ (batch size, time/sequence length, channel/text embedding size), multiply it with the Query, Key, and Value embedding matrix to get $q$,$k$,$v$ vectors of shape $B \\times T \\times d_k$. Where $d_k$ is the head size (size of each query, key, value vector).\n",
    "\n",
    "<div>\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "Use `nn.Linear` to define the `key`, `query`, and `value` embedding weights (take note to not include the bias). And calculate the `k`, `q`, and `v` vectors from $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:53:35.216984Z",
     "iopub.status.busy": "2025-01-22T15:53:35.216654Z",
     "iopub.status.idle": "2025-01-22T15:53:35.301537Z",
     "shell.execute_reply": "2025-01-22T15:53:35.300644Z",
     "shell.execute_reply.started": "2025-01-22T15:53:35.216958Z"
    },
    "id": "HoQqUTxGf-AE",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: 16, T: 256, C: 64, head_size: 16\n",
      "key: torch.Size([16, 64]), query: torch.Size([16, 64]), value: torch.Size([16, 64])\n",
      "tensor([ 0.7020,  0.6227, -0.2063, -1.0227,  0.5020, -1.4468,  0.7205, -0.3498,\n",
      "        -0.8498,  0.9094,  0.1168, -0.9637, -0.4064, -0.0979,  1.5379, -0.0558],\n",
      "       grad_fn=<SelectBackward0>) tensor([-1.0709, -1.1625,  0.0260,  0.3891, -0.5746,  0.1046, -0.5273,  0.1213,\n",
      "         1.1707,  0.2108,  0.4636,  0.3899,  1.4501, -0.0414,  0.9155,  0.0261],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([16, 256, 16]) torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# B, T, C = 1, 1, 16\n",
    "# head_size = 16\n",
    "x = torch.randn(B, T, C)\n",
    "print(f\"B: {B}, T: {T}, C: {C}, head_size: {head_size}\")\n",
    "\n",
    "#### FILL CODE HERE ####\n",
    "# Fill in these weight matrices\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "print(f\"key: {key.weight.shape}, query: {query.weight.shape}, value: {value.weight.shape}\")\n",
    "# key.weight.data = torch.eye(head_size, C)\n",
    "# query.weight.data = torch.eye(head_size, C)\n",
    "# value.weight.data = torch.eye(head_size, C)\n",
    "# print(f\"key: {key.weight.shape}\")\n",
    "\n",
    "# Calculate k,q,v vectors\n",
    "k = key(x)     # (B, T, d_k)\n",
    "q = query(x)   # (B, T, d_k)\n",
    "v = value(x)   # (B, T, d_k)\n",
    "######################\n",
    "print(k[0][0], q[0][0], k.shape, q.shape, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:07.660148Z",
     "iopub.status.busy": "2025-01-22T15:57:07.659776Z",
     "iopub.status.idle": "2025-01-22T15:57:07.673320Z",
     "shell.execute_reply": "2025-01-22T15:57:07.672437Z",
     "shell.execute_reply.started": "2025-01-22T15:57:07.660121Z"
    },
    "id": "6UMoK-A6nzIE",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "k_0_0 = torch.Tensor([0.7020,  0.6227, -0.2063, -1.0227,  0.5020, -1.4468,  0.7205, -0.3498, -0.8498,  0.9094,  0.1168, -0.9637, -0.4064, -0.0979,  1.5379, -0.0558])\n",
    "q_0_0 = torch.Tensor([-1.0709, -1.1625,  0.0260,  0.3891, -0.5746,  0.1046, -0.5273,  0.1213, 1.1707,  0.2108,  0.4636,  0.3899,  1.4501, -0.0414,  0.9155,  0.0261])\n",
    "print(torch.allclose(k_0_0, k[0][0].data, atol=1e-4, rtol=0))\n",
    "print(torch.allclose(q_0_0, q[0][0].data, atol=1e-4, rtol=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCDb0rYfHhkQ"
   },
   "source": [
    "### Q1: What is the first number of v[0][0]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MjVgWGl4Hj_G",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7776,  0.1929, -0.6711,  0.3857,  0.4316,  0.0201,  0.0026, -1.6102,\n",
       "        -0.1988,  0.3434,  1.1472, -0.4292, -0.5048, -0.9871,  0.2234,  0.8314],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BupSuPVNqwiP"
   },
   "source": [
    "### 1.2 Calculate Dot Product of Query and Value\n",
    "\n",
    "Perform dot product of `q` and `k` using `torch.mm` or `@` such that it has shape $B \\times T \\times T$. Do note that `transpose` is required for this to work, since both are at shape $B \\times T \\times d_k$. And normalize the resulting weights `wei` by $\\sqrt{d_k}$.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://jalammar.github.io/images/t/self-attention_softmax.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "Please take a look at the resulting `q` and `k` dot product. In a single batch, a `q` matrix has dimensions $T \\times d_k$ (each row represent the sequence length and columns are the embeddings of head size). We can view each row of `q` as the $\\vec{q}_1$ query vector represented above. The dot product would represent the following resulting matrix:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\color{red}{q_{1,1}} & \\color{red}{q_{1,2}} & \\color{red}{q_{1,3}} & \\color{red}\\cdots \\\\ q_{2,1} & q_{2,2} & q_{2,3} & \\cdots \\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix} \\color{blue}{k_{1,1}} & \\color{blue}{k_{1,2}} & \\color{blue}{k_{1,3}} & \\color{blue}\\cdots \\\\ k_{2,1} & k_{2,2} & k_{2,3} & \\cdots \\end{bmatrix}^T\n",
    "=\n",
    "\\begin{bmatrix} \\color{red}{\\vec{q}_1} \\cdot \\color{blue}{\\vec{k}_1} & \\color{red}{\\vec{q}_1} \\cdot \\vec{k}_2 \\\\ \\vec{q}_2 \\cdot \\color{blue}{\\vec{k}_1} & \\vec{q}_2 \\cdot \\vec{k}_2  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The resulting matrix would have the dimensions of $T \\times T$, where each row is the attention score of each word. For instance, referencing the image above, the first row is the attention scores of the first word \"Thinking\" compared with the other words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:08.766521Z",
     "iopub.status.busy": "2025-01-22T15:57:08.766230Z",
     "iopub.status.idle": "2025-01-22T15:57:08.836881Z",
     "shell.execute_reply": "2025-01-22T15:57:08.836219Z",
     "shell.execute_reply.started": "2025-01-22T15:57:08.766482Z"
    },
    "id": "9MICq8ZAqwT_",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7612,  0.2295,  0.0509,  0.2422, -0.0880,  0.7159,  0.3115,  0.1013],\n",
       "        [-0.1423,  0.0154,  0.1419,  0.1402, -0.0295,  0.3767, -0.2689,  0.2378],\n",
       "        [-0.2216, -0.0725, -0.2789, -0.1393, -0.0674,  0.4295,  0.0111, -0.1680],\n",
       "        [-0.0231, -0.1689, -0.3135,  0.2328, -0.0817,  0.0962, -0.0428, -0.5817],\n",
       "        [ 0.1118, -0.1521, -0.2316,  0.1784,  0.1115,  0.0956, -0.1409, -0.0464],\n",
       "        [-0.0497, -0.1134, -0.2892, -0.0801,  0.1453,  0.3804, -0.0347, -0.1634],\n",
       "        [ 0.3202,  0.2832, -0.1353,  0.3179,  0.2472,  0.1691, -0.1433,  0.2831],\n",
       "        [-0.1664, -0.0450, -0.5189, -0.1039,  0.4599, -0.2667,  0.1881,  0.5595]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FILL CODE HERE ####\n",
    "qT = q.transpose(1, 2) # (B, d_k, T)\n",
    "wei =  torch.matmul(k, qT) # (B, T, d_k) @ (B, d_k, T) ---> (B, T, T)\n",
    "wei = wei / (head_size**0.5)\n",
    "\n",
    "print(wei.shape)\n",
    "######################\n",
    "wei[0][:8, :8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_RHqPbOJS2v"
   },
   "source": [
    "### Q2: What shape is `wei` after the dot product of `q` and `k`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:14.890573Z",
     "iopub.status.busy": "2025-01-22T15:57:14.890277Z",
     "iopub.status.idle": "2025-01-22T15:57:14.894450Z",
     "shell.execute_reply": "2025-01-22T15:57:14.893329Z",
     "shell.execute_reply.started": "2025-01-22T15:57:14.890550Z"
    },
    "id": "7RUAqf5d8Aqz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# (16, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptQ6-2lfta6g"
   },
   "source": [
    "### 1.3 Perform Masked Attention with `torch.tril`\n",
    "\n",
    "Since we are making an autoregressive decoder-only block, it would be weird for the current token to be able to attend to future tokens. If we look at the figure above, it doesn't make any sense for the word \"Thinking\" to be able to see \"Machines\", else you already know the result to be generated. Hence, you need to \"mask\" these attentions.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "To do this, there is a special kind of matrix called triangular matrix. See the result of `torch.tril` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:15.226636Z",
     "iopub.status.busy": "2025-01-22T15:57:15.226313Z",
     "iopub.status.idle": "2025-01-22T15:57:15.236872Z",
     "shell.execute_reply": "2025-01-22T15:57:15.236007Z",
     "shell.execute_reply.started": "2025-01-22T15:57:15.226607Z"
    },
    "id": "XUechefjblXi",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(T,T))[:8,:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybmEPx38-Yrj"
   },
   "source": [
    "Referring to the resulting $Q \\cdot K$ of dimensions $T \\times T$, each row index represents the time dimension, and the columns are all the other words in the sequence. For instance, when we are at the current word \"Thinking\" at time $t=1$, the generation of the second word should not be able to to access $\\vec{q}_1 \\cdot \\vec{k}_1$, and not $\\vec{q}_1 \\cdot \\vec{k}_2$ (corresponding to keys of \"Machines\" and \"are\"). This is illustrated in the matrix below:\n",
    "\n",
    "$$\\require{cancel}$$\n",
    "$$\n",
    "\\begin{matrix} .\\qquad \\text{Thinking} & \\text{Machines} & \\text{are} \\end{matrix} \\\\\n",
    "\\begin{matrix} \\color{blue}{\\text{Thinking}} \\\\ \\text{Machines} \\\\ \\text{are} \\end{matrix}\n",
    "\\begin{bmatrix}  \\color{blue}{\\vec{q}_1 \\cdot \\vec{k}_1} & \\cancel{\\vec{q}_1 \\cdot \\vec{k}_2} & \\cancel{\\vec{q}_1 \\cdot \\vec{k}_3} \\\\ \\vec{q}_2 \\cdot \\vec{k}_1 & \\vec{q}_2 \\cdot \\vec{k}_2 & \\cancel{\\vec{q}_2 \\cdot \\vec{k}_3} \\\\ \\vec{q}_3 \\cdot \\vec{k}_1 & \\vec{q}_3 \\cdot \\vec{k}_2 & \\vec{q}_3 \\cdot \\vec{k}_3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is very similar to the triangular matrix. If we are able to filter out the attentions where `tril == 0`, masked self attention will be achieved.\n",
    "\n",
    "Use `masked_fill` on `wei` such that the resulting attention softmax is `0` just like the matrix above.  \n",
    "Note: $-\\infty$ or `float('-inf')` may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:15.545337Z",
     "iopub.status.busy": "2025-01-22T15:57:15.545057Z",
     "iopub.status.idle": "2025-01-22T15:57:15.561885Z",
     "shell.execute_reply": "2025-01-22T15:57:15.561150Z",
     "shell.execute_reply.started": "2025-01-22T15:57:15.545315Z"
    },
    "id": "mGv2qPjMnrA7",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4607, 0.5393, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3221, 0.3738, 0.3041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2562, 0.2214, 0.1916, 0.3308, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2200, 0.1689, 0.1560, 0.2351, 0.2199, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1551, 0.1455, 0.1221, 0.1504, 0.1885, 0.2384, 0.0000, 0.0000],\n",
       "        [0.1663, 0.1602, 0.1054, 0.1659, 0.1546, 0.1430, 0.1046, 0.0000],\n",
       "        [0.0984, 0.1111, 0.0692, 0.1047, 0.1840, 0.0890, 0.1403, 0.2033]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "#### FILL CODE HERE ####\n",
    "wei = torch.masked_fill(wei, tril == 0, float('-inf'))\n",
    "######################\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei[0][:8, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:16.719188Z",
     "iopub.status.busy": "2025-01-22T15:57:16.718883Z",
     "iopub.status.idle": "2025-01-22T15:57:16.726346Z",
     "shell.execute_reply": "2025-01-22T15:57:16.725539Z",
     "shell.execute_reply.started": "2025-01-22T15:57:16.719164Z"
    },
    "id": "Po5K7HhJI_az",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0][0][1:] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYHkY6_RKPmL"
   },
   "source": [
    "### 1.4 Calculate Dot Product of Softmax and Value\n",
    "\n",
    "Should be self-explanatory. We are at the final stage of the equation to get a resulting matrix of shape $B \\times T \\times d_k$ (we use the same dimensions for key and value).\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} )V\n",
    "$$\n",
    "\n",
    "<div>\n",
    "<img src=\"https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"500\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:17.183515Z",
     "iopub.status.busy": "2025-01-22T15:57:17.183232Z",
     "iopub.status.idle": "2025-01-22T15:57:17.189448Z",
     "shell.execute_reply": "2025-01-22T15:57:17.188427Z",
     "shell.execute_reply.started": "2025-01-22T15:57:17.183493Z"
    },
    "id": "A0ouoYAiI2bi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### FILL CODE HERE ####\n",
    "out = torch.matmul(wei, v) # (B, T, T) @ (B, T, d_k) ---> (B, T, d_k)\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76P5Pu2_Mo0k"
   },
   "source": [
    "### Q3: What shape is the resulting attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:18.253744Z",
     "iopub.status.busy": "2025-01-22T15:57:18.253412Z",
     "iopub.status.idle": "2025-01-22T15:57:18.258955Z",
     "shell.execute_reply": "2025-01-22T15:57:18.258076Z",
     "shell.execute_reply.started": "2025-01-22T15:57:18.253720Z"
    },
    "id": "S9QGn_ZaK6PP",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zhvJj79LiNi"
   },
   "source": [
    "### 1.5 Putting it all together!\n",
    "\n",
    "Now it's time to code the Attention Head `nn.Module`.\n",
    "1. Initialize all the `nn.Linear` in the constructor. Use the hyperparameter `n_embd` for the embedding size.\n",
    "2. Perform the self-attention calculations in the `forward` function\n",
    "\n",
    "Note that there may be some differences in the implemented version:\n",
    "- Since `tril` is not a parameter in the PyTorch module, it is registered as a `buffer` instead.\n",
    "- A `dropout` is appended after the softmax for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:18.718085Z",
     "iopub.status.busy": "2025-01-22T15:57:18.717755Z",
     "iopub.status.idle": "2025-01-22T15:57:18.724773Z",
     "shell.execute_reply": "2025-01-22T15:57:18.723728Z",
     "shell.execute_reply.started": "2025-01-22T15:57:18.718059Z"
    },
    "id": "wkpyS4Ff9qBN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        #### FILL CODE HERE ####\n",
    "        self.key = nn.Linear(C, head_size, bias=False)\n",
    "        self.query = nn.Linear(C, head_size, bias=False)\n",
    "        self.value = nn.Linear(C, head_size, bias=False)\n",
    "        ######################\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # print(f\"key: {self.key.weight.shape}, query: {self.query.weight.shape}, value: {self.value.weight.shape}\")\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, T, C = x.shape\n",
    "        #### FILL CODE HERE ####\n",
    "        k: Tensor = self.key(x)                   # (B, T, d_k)\n",
    "        q: Tensor = self.query(x)                 # (B, T, d_k)\n",
    "        v: Tensor = self.value(x)                 # (B, T, d_k)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        # qT = q.transpose(1, 2) # (B, d_k, T)\n",
    "        # wei = torch.matmul(k, qT) * (self.head_size**-0.5)               # Dot product of q * k & normalization (B, T, d_k) @ (B, d_k, T) -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5)   \n",
    "        \n",
    "        wei = torch.masked_fill(wei, self.tril[:T, :T] == 0, float('-inf'))          # Use masked_fill on tril (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)                                    # Apply softmax (B, T, T)\n",
    "        wei = self.dropout(wei)                                         # Added dropout\n",
    "        out = torch.matmul(wei, v)                                      # (B, T, T) @ (B, T, d_k) -> (B, T, d_k)\n",
    "        ######################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj4hooukM8Sc"
   },
   "source": [
    "### Q4: What is Head's output shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:19.075128Z",
     "iopub.status.busy": "2025-01-22T15:57:19.074813Z",
     "iopub.status.idle": "2025-01-22T15:57:19.102265Z",
     "shell.execute_reply": "2025-01-22T15:57:19.101408Z",
     "shell.execute_reply.started": "2025-01-22T15:57:19.075104Z"
    },
    "id": "bpXKLTGiLMQd",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(B,T,C)\n",
    "head = Head(head_size)\n",
    "#### FILL CODE HERE ####\n",
    "out = head(x) \n",
    "out.shape\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILFCwZEkOLof"
   },
   "source": [
    "## Part 2: Multi-Head Attention\n",
    "\n",
    "This part implements the 3.2.2 Multi-Head Attention in the paper _Attention is All You Need`.\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ... ,\\text{head}_h)W^O\\\\\n",
    "\\text{where}\\: \\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzzokhk_C1Qz"
   },
   "source": [
    "With multiple heads running in parallel, this would give rise to multiple representation subspaces, where each head would have multiple sets of Q/K/V matrices.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "The resulting attention would be all concatenated to a long matrix.\n",
    "\n",
    "To preserve the shape of the vector back to the embedding size $C$ before the feed-forward layer (if the concatenation does not have the same size as the embedding), we use a projection layer $W^O$ with dimensions $hd_k \\times C$.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "There are two things you need to implement:\n",
    "- The `self.heads` which is an `nn.ModuleList` of all the Attention `Head` of $h$ (`num_heads`) layers (these will be computed in parallel). And `torch.cat` to concatenante the heads in `forward`.\n",
    "- The `self.proj` projection layer $W^O$ (with dimensions $C \\times C$ as noted below). Use the hyperparameter `n_embd` for the embedding size. Apply the projection accordingly in `forward`.\n",
    "\n",
    "Note that in the paper, they use $d_k = C/h = 64$. The head size is equal to the embedding size divided by the number of heads. So the higher number of heads, the lower the dimension of the head to preserve computational cost equivalent to single head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:21.289795Z",
     "iopub.status.busy": "2025-01-22T15:57:21.289439Z",
     "iopub.status.idle": "2025-01-22T15:57:21.295266Z",
     "shell.execute_reply": "2025-01-22T15:57:21.294293Z",
     "shell.execute_reply.started": "2025-01-22T15:57:21.289765Z"
    },
    "id": "XVVLYJs_OR5U",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): # checked\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        #### FILL CODE HERE ####\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        ######################\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### FILL CODE HERE ####\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1) # (B, T, d_k) * num_heads -> (B, T, d_k*num_heads)\n",
    "        out = self.proj(out)                                      # (B, T, d_k*num_heads) -> (B, T, C)\n",
    "        ######################\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpIrUDkZN8lL"
   },
   "source": [
    "### Q5: What is MultiHead's output shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:22.508841Z",
     "iopub.status.busy": "2025-01-22T15:57:22.508554Z",
     "iopub.status.idle": "2025-01-22T15:57:22.575209Z",
     "shell.execute_reply": "2025-01-22T15:57:22.574449Z",
     "shell.execute_reply.started": "2025-01-22T15:57:22.508819Z"
    },
    "id": "JKMC83IJVHZk",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(B,T,C) # C = 64\n",
    "heads = MultiHeadAttention(n_head, head_size) # 4 heads, each of size 16\n",
    "#### FILL CODE HERE ####\n",
    "out = heads(x)\n",
    "out.shape\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21lJ8cn8P_Vm"
   },
   "source": [
    "## Part 3: Feed Forward\n",
    "\n",
    "At each block, there is a fully connected feed-forward network. Implement the 3.3 Position-wise Feed-Forward Networks in the paper _Attention is All You Need_.\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1+b_1)W_2 + b_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5JbegqUUJuv"
   },
   "source": [
    "This part gets the resulting matrix of shape $B \\times T \\times C$ from the Multi-Head Attention. The paper noted that they used an embedding size dimensionality of $C=512$ and the feed-forward inner-layer dimensionality $d_{ff}=2048$, which is pretty much $4C$.\n",
    "\n",
    "Implement the Feed-forward equation up top with `nn.Linear` and `nn.ReLU` with the correct embedding size. Use `n_embd` for embedding size. Preserve the shape of $B \\times T \\times C$ in the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:25.142688Z",
     "iopub.status.busy": "2025-01-22T15:57:25.142397Z",
     "iopub.status.idle": "2025-01-22T15:57:25.147408Z",
     "shell.execute_reply": "2025-01-22T15:57:25.146469Z",
     "shell.execute_reply.started": "2025-01-22T15:57:25.142665Z"
    },
    "id": "MQzzhaynP-vW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module): # checked\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            #### FILL CODE HERE ####\n",
    "            nn.Linear(n_embd, n_embd*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd),\n",
    "            ######################\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:25.324348Z",
     "iopub.status.busy": "2025-01-22T15:57:25.324095Z",
     "iopub.status.idle": "2025-01-22T15:57:25.340840Z",
     "shell.execute_reply": "2025-01-22T15:57:25.340084Z",
     "shell.execute_reply.started": "2025-01-22T15:57:25.324327Z"
    },
    "id": "X-HQV2C-VDBq",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(B,T,C)\n",
    "_module = FeedFoward(n_embd) # C = n_embd\n",
    "out = _module(x)\n",
    "print(out.shape)\n",
    "out.shape == torch.Size([B,T,n_embd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAC-UdkXO54K"
   },
   "source": [
    "### Q6: How many parameters are in a FeedForward module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:25.665425Z",
     "iopub.status.busy": "2025-01-22T15:57:25.665189Z",
     "iopub.status.idle": "2025-01-22T15:57:25.670485Z",
     "shell.execute_reply": "2025-01-22T15:57:25.669818Z",
     "shell.execute_reply.started": "2025-01-22T15:57:25.665404Z"
    },
    "id": "V1rW9x2UO7g2",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33088"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in _module.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKqz0rrqQRF3"
   },
   "source": [
    "## Part 4: Transformer Block\n",
    "\n",
    "Putting all the blocks together! We have initialize the constructor with all the defined previous components along with `nn.LayerNorm`.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" width=\"400\" />\n",
    "</div>\n",
    "**Note: DO NOT reference this image**\n",
    "\n",
    "Now I would like you to implement the residual connections, self-attention, feed forward, and the layer norm in `forward`. As a slight deviation from the paper, now it is more common to do pre-norm, which is to apply `LayerNorm` before self-attention.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1QnTkcVlyoseiZk65-IHbQhESpwmX6Zfx\" width=\"400\" />\n",
    "</div>\n",
    "\n",
    "1. Apply the first layer norm to $x$, put it through self-attention layer, and add in the residual connection.\n",
    "2. Apply the second layer norm, put it through feed forward layer, and add in the residual connection.\n",
    "\n",
    "Reference: [Andrej Kaparty - Let's build GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY).   \n",
    "[Prenorm](https://arxiv.org/pdf/2002.04745)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:27.175593Z",
     "iopub.status.busy": "2025-01-22T15:57:27.175323Z",
     "iopub.status.idle": "2025-01-22T15:57:27.180667Z",
     "shell.execute_reply": "2025-01-22T15:57:27.179751Z",
     "shell.execute_reply.started": "2025-01-22T15:57:27.175572Z"
    },
    "id": "IJdorNIAQRfA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module): # checked\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### FILL CODE HERE ####\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        ######################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYaGUls7QhZ5"
   },
   "source": [
    "## Part 5: Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:27.483597Z",
     "iopub.status.busy": "2025-01-22T15:57:27.483353Z",
     "iopub.status.idle": "2025-01-22T15:57:27.491813Z",
     "shell.execute_reply": "2025-01-22T15:57:27.490890Z",
     "shell.execute_reply.started": "2025-01-22T15:57:27.483578Z"
    },
    "id": "oql3z2JGQgUC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # n_embd: 64\n",
    "        self.position_embedding_table = nn.Embedding(seq_len, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # n_layer: 4\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        assert not torch.isnan(tok_emb).any(), \"token NaN detected!\"\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        assert not torch.isnan(tok_emb).any(), \"pos NaN detected!\"\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x)    # (B,T,C)\n",
    "        x = self.ln_f(x)      # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -seq_len:]            # crop idx to the last block_size tokens\n",
    "            logits, loss = self(idx_cond)           # get the predictions\n",
    "            logits = logits[:, -1, :]               # focus only on the last time step - becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)       # apply softmax to get probabilities - (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution - (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # append sampled index to the running sequence - (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:29.203866Z",
     "iopub.status.busy": "2025-01-22T15:57:29.203501Z",
     "iopub.status.idle": "2025-01-22T15:57:29.227513Z",
     "shell.execute_reply": "2025-01-22T15:57:29.226691Z",
     "shell.execute_reply.started": "2025-01-22T15:57:29.203829Z"
    },
    "id": "TMgeIGxiSqDv",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.224839 M parameters\n"
     ]
    }
   ],
   "source": [
    "class TransformerLMModule(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = TransformerLanguageModel()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        xb, yb = batch\n",
    "        # evaluate the loss\n",
    "        logits, loss = self.model(xb, yb)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        xb, yb = val_batch\n",
    "        logits, loss = self.model(xb, yb)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        if batch_idx % self.trainer.log_every_n_steps == 0:\n",
    "            now = datetime.now()\n",
    "            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx}/{self.trainer.max_steps} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')\n",
    "\n",
    "L.pytorch.seed_everything(42)\n",
    "m = TransformerLMModule()\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # print the number of parameters in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J48ANUs4QHOX"
   },
   "source": [
    "### Q7: How many parameters are in your nanoGPT model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T08:53:22.142476Z",
     "iopub.status.busy": "2025-01-22T08:53:22.142135Z",
     "iopub.status.idle": "2025-01-22T08:59:20.488616Z",
     "shell.execute_reply": "2025-01-22T08:59:20.487901Z",
     "shell.execute_reply.started": "2025-01-22T08:53:22.142446Z"
    },
    "id": "PFCvaRPkjFOd",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "  | Name  | Type                     | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model | TransformerLanguageModel | 224 K  | train\n",
      "-----------------------------------------------------------\n",
      "224 K     Trainable params\n",
      "0         Non-trainable params\n",
      "224 K     Total params\n",
      "0.899     Total estimated model params size (MB)\n",
      "138       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 4.4270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdf2554caa2470d8fef6a0cd3288e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22T08:53:22 Step: 0/5000 Train Loss: 4.4157"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 3.0635\n",
      "2025-01-22T08:53:40 Step: 250/5000 Train Loss: 3.0362"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.9848\n",
      "2025-01-22T08:53:58 Step: 500/5000 Train Loss: 2.9529"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.7778\n",
      "2025-01-22T08:54:16 Step: 750/5000 Train Loss: 2.7213"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.6108\n",
      "2025-01-22T08:54:34 Step: 1000/5000 Train Loss: 2.6015"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.4784\n",
      "2025-01-22T08:54:52 Step: 1250/5000 Train Loss: 2.4182"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.3874\n",
      "2025-01-22T08:55:10 Step: 1500/5000 Train Loss: 2.3487"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.3161\n",
      "2025-01-22T08:55:28 Step: 1750/5000 Train Loss: 2.3361"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.2665\n",
      "2025-01-22T08:55:45 Step: 2000/5000 Train Loss: 2.2796"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.2292\n",
      "2025-01-22T08:56:03 Step: 2250/5000 Train Loss: 2.2141"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.1900\n",
      "2025-01-22T08:56:21 Step: 2500/5000 Train Loss: 2.0409"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.1663\n",
      "2025-01-22T08:56:39 Step: 2750/5000 Train Loss: 2.1309"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.1317\n",
      "2025-01-22T08:56:57 Step: 3000/5000 Train Loss: 2.0846"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.1077\n",
      "2025-01-22T08:57:15 Step: 3250/5000 Train Loss: 2.1207"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0877\n",
      "2025-01-22T08:57:33 Step: 3500/5000 Train Loss: 1.9982"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0725\n",
      "2025-01-22T08:57:50 Step: 3750/5000 Train Loss: 1.9788"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0567\n",
      "2025-01-22T08:58:08 Step: 4000/5000 Train Loss: 1.9801"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0555\n",
      "2025-01-22T08:58:26 Step: 4250/5000 Train Loss: 1.9852"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0341\n",
      "2025-01-22T08:58:44 Step: 4500/5000 Train Loss: 1.8977"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0123\n",
      "2025-01-22T08:59:02 Step: 4750/5000 Train Loss: 1.9681"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0104\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(deterministic=True, accelerator=\"auto\", devices=\"auto\",  logger=False, \\\n",
    "                    max_steps=max_iters,\n",
    "                    val_check_interval = eval_interval,\n",
    "                    log_every_n_steps =eval_interval,\n",
    "                    enable_checkpointing =False,\n",
    "                    limit_val_batches = eval_iters)\n",
    "trainer.fit(m, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UMES7hETe06"
   },
   "source": [
    "### Q8: What's the perplexity (from validation loss) on Step 5000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5G655a_8TipY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7.4663\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "val_loss = 2.0104\n",
    "perplexity = math.exp(val_loss)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LpwBtAOTlDi"
   },
   "source": [
    "### Q9: What's your output from the generated text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T09:04:27.255001Z",
     "iopub.status.busy": "2025-01-22T09:04:27.254710Z",
     "iopub.status.idle": "2025-01-22T09:04:45.693519Z",
     "shell.execute_reply": "2025-01-22T09:04:45.692723Z",
     "shell.execute_reply.started": "2025-01-22T09:04:27.254981Z"
    },
    "id": "xAbQtCkEh4UG",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๏ อาจารย์พีรพลสอนเอ็นแอลพี\tสะฉุนชีวันเห็นผู้พูด้วยคลี\n",
      "มีสมพวกปั้นมันลังกันอี่ดี\tเที่ยวลงล้าไทนพึงให้หอนตาติดกล\n",
      "เอาพาผูญให้แจ้งต่างวิ่งทัพน\tถึงก็าตยดทั้งก็กิจะบึงติดดา\n",
      "เขามวิชดจำแซ่ม่ไม่อยู่กัน\tชอบทิ้งด้วนหยิบสัยใช้ใกล้ใแปลง\n",
      "โฉมยงหลงลืมยื้มชื่อหอถน้ำ\tหน้าสำฆ้องเมียงามนักรำพัน\n",
      "ทั้งกราบกฎพันท์งามารี\tรู้หน้าพูดปกันดีซูงก็มัวให้\n",
      "เมืองเหลืองามาหรือทำภิปัลง\tพึ่งทำยังบิต์ซูกพิษพักงาม\n",
      "ด้วยรายนายสารพักตร\n",
      "เข้ารบรัสหัสตัดคดคงคา\tร้องค์ครูปกรอิ้นหน่าโจมธิ์หัดทา\n",
      "นางนางยนภูมาพระญาอยู่พราย\tตรู้ตูตตาแทงพงศ์ไม่กรรมจา\n",
      "ขอมพริ้มพรั่งแรงได้ก่อนประดับ\tมันสาโพรยมแศกนตรี ฯ\n",
      "๏ อ้าคิดสาไปถึงไพร่พลัดก่าช่\tพาสมนตราบกัมผลิ้มชลายว่าจาบ\n",
      "มันโกขฟังเล่าอยู่จะเราวี\tพระชนหนับนี้สี่รูปกันบสีประมัดสดา\n",
      "เฝี่ยวข้าเข้าเฝ้าแล้วชาติวงศ์\tจนรำลับสลับแคลงความใจ ฯ\n",
      "๏ ฟังยังกษรรู้ผ้าใหญ่สงสัย\tโดกรธโสราเหร่งซ้ำตั้นรองใน ฯ\n",
      "๏ ฝ่ายยู่ใครจริบหอยไม่เนื่อ\tลูกหรือแอบแยบมาถึงพ่าสงสัย\n",
      "ท้าวสนามลุบัตส่าอ้ายหุดก้อมทัพ\tใช่จึงชิดตามไปรักษา\n",
      "ไม่ขักษ์ขานอย่างแต่ปางขนิ่ง\tทำเป็นเปลินฆ้องต้องในถลง\n",
      "เสนารับกราชกำนาญขึ้นปรักษาสัย\tอันกริ้นองรองค์ลงลนักษือ\n",
      "จะต่อพระองค์ปลงลาชวนบิ้นกล\n",
      "จะยิบน\n"
     ]
    }
   ],
   "source": [
    "L.pytorch.seed_everything(42)\n",
    "# generate from the model\n",
    "context = torch.tensor([encode(\"๏ อาจารย์พีรพลสอนเอ็นแอลพี\t\")], dtype=torch.long, device=device)\n",
    "print(decode(m.model.to(device).generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5ND0b5nVUvN"
   },
   "source": [
    "## Part 6: Transformer from HuggingFace\n",
    "\n",
    "In this part you will be using `transformers` from HuggingFace, the go-to library for many models. We will be, in similar fashion, training DistilGPT2 on the same dataset. I have provided you the tokenizer and Dataloader for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:57:58.286307Z",
     "iopub.status.busy": "2025-01-22T15:57:58.286022Z",
     "iopub.status.idle": "2025-01-22T15:57:59.731112Z",
     "shell.execute_reply": "2025-01-22T15:57:59.730409Z",
     "shell.execute_reply.started": "2025-01-22T15:57:58.286286Z"
    },
    "id": "RqoVrwdmVUBe",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0136caf07c6d4aab83d4ae693f771aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d42f6e43dc489aaef1cc7fb7c72e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8b9da8a46042da966b32137428bef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03fccbabe414ec7896e52c97338c42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2702b643e0747d8ab46518a0c7676d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T09:04:47.679505Z",
     "iopub.status.busy": "2025-01-22T09:04:47.679189Z",
     "iopub.status.idle": "2025-01-22T09:04:47.684736Z",
     "shell.execute_reply": "2025-01-22T09:04:47.684048Z",
     "shell.execute_reply.started": "2025-01-22T09:04:47.679481Z"
    },
    "id": "358YrYXlV9Q2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\n",
      "The beginning of sequence token <|startoftext|> token has the id 50257\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The padding token <|pad|> has the id 50258\n"
     ]
    }
   ],
   "source": [
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:03:50.272589Z",
     "iopub.status.busy": "2025-01-22T16:03:50.272276Z",
     "iopub.status.idle": "2025-01-22T16:03:53.451488Z",
     "shell.execute_reply": "2025-01-22T16:03:53.450687Z",
     "shell.execute_reply.started": "2025-01-22T16:03:50.272564Z"
    },
    "id": "Ohb0NHQOWB22",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415 157\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "seq_len=768\n",
    "batch_size=8\n",
    "\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(text)) # first 90% will be train, rest val\n",
    "train_text = text[:n]\n",
    "val_text = text[n:]\n",
    "\n",
    "class GPT2TextDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, text, tokenizer, seq_len=768):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "    for i in range(0, len(text)//700):\n",
    "      encodings_dict = tokenizer('<|startoftext|>'+ text[i*700:(i+1)*700] + '<|endoftext|>', truncation=True, max_length=seq_len, padding=\"max_length\")\n",
    "\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "train_dataset = GPT2TextDataset(train_text, tokenizer, seq_len)\n",
    "val_dataset = GPT2TextDataset(val_text, tokenizer, seq_len)\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "print(train_dataset[0][0].shape)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lfz2LJijWY9L"
   },
   "source": [
    "Read how Auto Classes work in HuggingFace https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes , and use `AutoModelForCausalLM.from_pretrained` to initialize your `distilgpt2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:03:54.653128Z",
     "iopub.status.busy": "2025-01-22T16:03:54.652818Z",
     "iopub.status.idle": "2025-01-22T16:04:12.457894Z",
     "shell.execute_reply": "2025-01-22T16:04:12.457008Z",
     "shell.execute_reply.started": "2025-01-22T16:03:54.653105Z"
    },
    "id": "pPWfz-UaWEOO",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe7612b1620425d93c494be537579b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663841ee92b84d37ab32cf9724374a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d0d076a59f47cbb742aa1a9866edee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.914112 M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "class DistilGPT2(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #### FILL CODE HERE ####\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "        ######################\n",
    "        self.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-4)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        xb, mask = batch\n",
    "        # evaluate the loss\n",
    "        loss, logits = self.model(xb, labels=xb, attention_mask=mask, token_type_ids=None)[:2]\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        xb, mask = val_batch\n",
    "        loss, logits = self.model(xb, labels=xb, attention_mask=mask, token_type_ids=None)[:2]\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        if batch_idx % self.trainer.log_every_n_steps == 0:\n",
    "            now = datetime.now()\n",
    "            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        beginning_text = tokenizer('<|startoftext|>'+\"แต่ปางหลังยังมีกรุงกษัตริย์\", return_tensors=\"pt\").to(device)\n",
    "        sample_outputs = self.model.generate(beginning_text['input_ids'], attention_mask=beginning_text[\"attention_mask\"],pad_token_id=tokenizer.pad_token_id, do_sample=True, max_length =300, num_return_sequences=1)\n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "              print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True).strip()))\n",
    "\n",
    "L.pytorch.seed_everything(42)\n",
    "distilgpt2 = DistilGPT2()\n",
    "print(sum(p.numel() for p in distilgpt2.parameters())/1e6, 'M parameters') # print the number of parameters in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTf-fav6XIne"
   },
   "source": [
    "### Q10: How many parameters are in the DistilGPT2 model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7UG1UIScyjD"
   },
   "source": [
    "<font color='red'>Training should take around ~18 minutes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:06:21.393140Z",
     "iopub.status.busy": "2025-01-22T16:06:21.392290Z",
     "iopub.status.idle": "2025-01-22T16:25:46.695310Z",
     "shell.execute_reply": "2025-01-22T16:25:46.694375Z",
     "shell.execute_reply.started": "2025-01-22T16:06:21.393095Z"
    },
    "id": "btFuxXQWXIXw",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "  | Name  | Type            | Params | Mode\n",
      "-------------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 81.9 M | eval\n",
      "-------------------------------------------------\n",
      "81.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "81.9 M    Total params\n",
      "327.656   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "86        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.2236\n",
      "0: แต่ปางหลังยังมีกรุงกษัตริย์วไอฃีแหแีรุแดุ่ปางหแุอฃีแาน่ตํทคทถฮุยํราไอฃีแุอฃีแอแีแาน่็ดุ่ปางหแีแาน่ตํทคทคทคทคทคทคทครุแณทควลกรุอฃีแาน่ตํทคทคทคทคทคทคทคท�\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e8d59745834fe19e166f7c914afaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22T16:06:26 Step: 0 Train Loss: 2.2306"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.6977\n",
      "0: แต่ปางหลังยังมีกรุงกษัตริย์ว่องีแห้มรัแดน์หสี\n",
      "เพไลาตป่าทย่างอดามำทะข่ายไรศะยังตาสองนักสวต่ืง\tองเจะชนากี็ขง้งด้หน่อแรอไณน\n",
      "ึกวญัม้งย๏\n",
      "มชี้วพยตใสทหรวกรทอี�\n",
      "2025-01-22T16:10:05 Step: 0 Train Loss: 1.6690"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.3818\n",
      "0: แต่ปางหลังยังมีกรุงกษัตริย์ทรง\n",
      "แพระอแกล่งหรสานร\tดพรณจำะันหนกราดาน ฯ\n",
      "๏ นางที่ข้าบิงทัน์หายหญังนท่อก\tจะให้นมปคะรุดัสุกรจะดี\n",
      "ปาใครพีสงพรัย้งกำันพุกล\tใหญำแสึ�\n",
      "2025-01-22T16:13:59 Step: 0 Train Loss: 1.3309"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.2072\n",
      "0: แต่ปางหลังยังมีกรุงกษัตริย์\n",
      "แม้นคว้อ่าออมแประแลาบอกเปร่ว\tแขงล้งกลับคลิกลุกผ์พอยสัย ฯ\n",
      "๏ นุศโกสนรเฝ้าหญิงนเฝ้าแสน\tพระแยบกับข้าเถิดดตายุบได้\n",
      "พระกละซ้องเลือย\n",
      "2025-01-22T16:17:55 Step: 0 Train Loss: 1.1624"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.1017\n",
      "0: แต่ปางหลังยังมีกรุงกษัตริย์\tสังห์สิบสเจ้าเห็นความตามเลี้ยวมา\n",
      "มาฤรพี่นางฟีพหลีได้สมนาโล่า\tอยังโสรรไพร่ใครจอกจัน ฯ\n",
      "๏ สิ้นที่ล้อมลมจริงจา\tตะพัพงศดพลหลั่นห่า\n",
      "2025-01-22T16:21:50 Step: 0 Train Loss: 0.9567"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.0619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: แต่ปางหลังยังมีกรุงกษัตริย์\tไปยิพลอดนางพลัดมารศรี\n",
      "บ�นหลานรีบหัตถ์ลัทธิ์จุดัย\tถึงจะแอมารด้วยมีพพรากัน\n",
      "ดูแล้วหาอย้อยู่พรางคูณี\tจรูปอู่พลับไม่ห้ากำลัง\n",
      "จึงว่\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(deterministic=True, accelerator=\"auto\", devices=\"auto\",  logger=False, \\\n",
    "                    max_epochs=5,\n",
    "                    log_every_n_steps =len(train_dataloader),\n",
    "                    enable_checkpointing =False,\n",
    "                    )\n",
    "trainer.fit(distilgpt2, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fcy3dcGc2RM"
   },
   "source": [
    "### Q11: What's the perplexity (from validation loss) on the last step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:40:28.725105Z",
     "iopub.status.busy": "2025-01-22T16:40:28.724704Z",
     "iopub.status.idle": "2025-01-22T16:40:28.730042Z",
     "shell.execute_reply": "2025-01-22T16:40:28.729227Z",
     "shell.execute_reply.started": "2025-01-22T16:40:28.725072Z"
    },
    "id": "qnHTZjgJc5yo",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.8919\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "val_loss = 1.0619\n",
    "perplexity = math.exp(val_loss)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUcvTGYMc6ee"
   },
   "source": [
    "### Q12: What's the output from the generated text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:40:43.514133Z",
     "iopub.status.busy": "2025-01-22T16:40:43.513627Z",
     "iopub.status.idle": "2025-01-22T16:40:54.006854Z",
     "shell.execute_reply": "2025-01-22T16:40:54.006109Z",
     "shell.execute_reply.started": "2025-01-22T16:40:43.514093Z"
    },
    "id": "eh6UNBw6c8TO",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "อาจารย์พีรพลสอนเอ็นแอลพี\tจะได้อนใจก็เป็นการะสราญรา\n",
      "ค่อยล้อมพลอยขึ้นสมุทร\tพวกโรธีใจห้ันจะครวญคา\n",
      "ให้ยวิมลีปิ่ดให้หว้าฆ�ใจสครอ\tแทกวดทัพพี่น้องทัพ\n",
      "พออ่านเรศน์อันตร์แจ่งการ\tไม่หกูไตใจครอยคอยรัก\n",
      "ทั้งโรเหมือเกลื่องหงเลืองอกให้ทำ\tได้แจะฉ้วงอยู่จะแผ่นเข่นยั้น\n",
      "ถ้าว\n"
     ]
    }
   ],
   "source": [
    "L.pytorch.seed_everything(40)\n",
    "beginning_text = tokenizer('<|startoftext|>'+\"อาจารย์พีรพลสอนเอ็นแอลพี\", return_tensors=\"pt\")\n",
    "output = distilgpt2.model.generate(beginning_text['input_ids'], attention_mask=beginning_text[\"attention_mask\"],pad_token_id=tokenizer.pad_token_id, do_sample=True, max_length =500, num_return_sequences=1)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
