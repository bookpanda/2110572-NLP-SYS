{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vdx6X6c8kJn"
      },
      "source": [
        "# HW3: Transformer from Scratch\n",
        "\n",
        "In this exercise, you are replicating character-level transformer from scratch with Pytorch Lightning. You should end up with similar code to [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "We have prepared for you a dataset and dataloader of พระอภัยมณี by สุนทรภู่ , a famous Thai poet. You should receive your very own nanoสุนทรภู่ by the end of this exercise.\n",
        "\n",
        "Reference: [Andrej Kaparty - Let's build GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY).  \n",
        "Data Source: [Vajirayana - พระอภัยมณี](https://vajirayana.org/%E0%B8%9E%E0%B8%A3%E0%B8%B0%E0%B8%AD%E0%B8%A0%E0%B8%B1%E0%B8%A2%E0%B8%A1%E0%B8%93%E0%B8%B5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u544827zmHRK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "--2025-01-21 15:30:50--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt [following]\n",
            "--2025-01-21 15:30:51--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3231076 (3.1M) [application/octet-stream]\n",
            "Saving to: ‘pra-apai-manee-ch1-50.txt’\n",
            "\n",
            "pra-apai-manee-ch1- 100%[===================>]   3.08M  8.17MB/s    in 0.4s    \n",
            "\n",
            "2025-01-21 15:30:52 (8.17 MB/s) - ‘pra-apai-manee-ch1-50.txt’ saved [3231076/3231076]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip -q install lightning\n",
        "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "hRBXmNByL5hP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x138875610>"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.types import Tensor\n",
        "import lightning as L\n",
        "from datetime import datetime\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # B: how many independent sequences will we process in parallel?\n",
        "seq_len = 256    # T: what is the maximum context length for predictions?\n",
        "n_embd = 64     # C: text embedding size\n",
        "n_head = 4      # number of heads\n",
        "n_layer = 4     # number of blocks\n",
        "max_iters = 5000\n",
        "eval_interval = 250\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "JE7GsqOy7Dzg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset in characters:  1100605\n",
            "๏ แต่ปางหลังยังมีกรุงกษัตริย์\n",
            "สมมุติวงศ์ทรงนามท้าวสุทัศน์\tผ่านสมบัติรัตนานามธานี\n",
            "อันกรุงไกรใหญ่ยาวสิบเก้าโยชน์\tภูเขาโขดเป็นกำแพงบุรีศรี\n",
            "สะพรึบพร้อมไพร่ฟ้าประชาชี\tชาวบุรีหรรษาสถาวร\n",
            "มีเอกองค์นงลักษณ์อัครราช\tพระนางนาฏนามปทุมเกสร\n",
            "สนมนางแสนสุรางคนิกร\tดังกินนรน่ารักลักขณา\n",
            "มีโอรสสององค์ล้วนทรงลักษณ์\tประไพพักตร์เพียงเทพเลขา\n",
            "ชื่ออภัยมณีเป็นพี่ยา\tพึ่งแรกรุ่นชันษาสิบห้าปี\n",
            "อันกุมารศรีสุวรรณนั้นเป็นน้อง\tเนื้อดังทองนพคุณจำรูญศรี\n",
            "พึ่งโสกันต์ชันษาสิบสามปี\tพระชนนีรักใคร่ดังนัยนา\n",
            "สมเด็จท้าวบิตุรงค์ดำรงราชย์\tแสนสวาทลูกน้อยเสน่หา\n",
            "จะเสกสองครองสมบัติขัตติยา\tแต่วิชาสิ่งใดไม่ชำนาญ\n",
            "จึงดำรัสเรียกพระโอรสราช\tมาริมอาสน์แท่นสุวรรณแล้วบรรหาร\n",
            "พ่อจะแจ้งเจ้าจงจำคำโบราณ\tอันชายชาญเชื้อกษัตริย์ขัตติยา\n",
            "ย่อมพากเพียรเรียนไสยศาสตร์เวท\tสิ่งวิเศษสืบเสาะแสวงหา\n",
            "ได้ป้องกันอันตรายนครา\tตามกษัตริย์ขัตติยาอย่างโบราณ\n",
            "พระลูกรักจักสืบวงศ์กษัตริย์\tจงรีบรัดเสาะแสวงแห่งสถาน\n",
            "หาทิศาปาโมกข์ชำนาญชาญ\tเป็นอาจารย์พากเพียรเรียนวิชา ฯ\n",
            "๏ บัดนั้นพี่น้องสองกษัตริย์\tประนมหัตถ์อภิวันท์ด้วยหรรษา\n",
            "จึงทูลความตามจิตเจตนา\tลูกคิดมาจะประมาณก็นานครัน\n",
            "หวังแสวงไปตำ\n"
          ]
        }
      ],
      "source": [
        "with open('pra-apai-manee-ch1-50.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "x2wvWIFaqbzb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Characters: \t\n",
            " กขคฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮฯะัาำิีึืุูเแโใไๅ็่้๊๋์๏\n",
            "Vocab Size: 71\n",
            "[42, 39, 49, 42, 20, 53, 5, 35, 49, 26]\n",
            "สวัสดีครับ\n"
          ]
        }
      ],
      "source": [
        "# Quick implementation of character tokenizer\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"All Characters: {''.join(chars)}\")\n",
        "print(f\"Vocab Size: {vocab_size}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"สวัสดีครับ\"))\n",
        "print(decode(encode(\"สวัสดีครับ\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "GWBxs6HNuYun"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1100605]) torch.int64\n",
            "(tensor([70,  2, 59, 21, 65, 27, 50,  7, 43, 37, 49,  7, 34, 49,  7, 33, 53,  3,\n",
            "        35, 56,  7,  3, 41, 49, 21, 35, 52, 34, 69,  1, 42, 33, 33, 56, 21, 52,\n",
            "        39,  7, 40, 69, 23, 35,  7, 25, 50, 33, 23, 66, 50, 39, 42, 56, 23, 49,\n",
            "        40, 25, 69,  0, 28, 65, 50, 25, 42, 33, 26, 49, 21, 52, 35, 49, 21, 25,\n",
            "        50, 25, 50, 33, 24, 50, 25, 53,  1, 45, 49, 25,  3, 35, 56,  7, 62,  3,\n",
            "        35, 61, 43, 13, 65, 34, 50, 39, 42, 52, 26, 58,  3, 66, 50, 60, 34, 10,\n",
            "        25, 69,  0, 32, 57, 58,  4, 50, 60,  4, 20, 58, 27, 64, 25,  3, 51, 59,\n",
            "        30,  7, 26, 56, 35, 53, 40, 35, 53,  1, 42, 48, 30, 35, 54, 26, 30, 35,\n",
            "        66, 45, 33, 62, 30, 35, 65, 31, 66, 50, 27, 35, 48, 10, 50, 10, 53,  0,\n",
            "        10, 50, 39, 26, 56, 35, 53, 43, 35, 35, 41, 50, 42, 22, 50, 39, 35,  1,\n",
            "        33, 53, 58, 45,  3, 45,  7,  5, 69, 25,  7, 37, 49,  3, 41, 19, 69, 45,\n",
            "        49,  5, 35, 35, 50, 10,  0, 30, 35, 48, 25, 50,  7, 25, 50, 15, 25, 50,\n",
            "        33, 27, 23, 56, 33, 58,  3, 42, 35,  1, 42, 25, 33, 25, 50,  7, 59, 42,\n",
            "        25, 42, 56, 35, 50,  7,  5, 25, 52,  3, 35,  0, 20, 49,  7,  3, 52, 25,\n",
            "        25, 35, 25, 65]), tensor([ 2, 59, 21, 65, 27, 50,  7, 43, 37, 49,  7, 34, 49,  7, 33, 53,  3, 35,\n",
            "        56,  7,  3, 41, 49, 21, 35, 52, 34, 69,  1, 42, 33, 33, 56, 21, 52, 39,\n",
            "         7, 40, 69, 23, 35,  7, 25, 50, 33, 23, 66, 50, 39, 42, 56, 23, 49, 40,\n",
            "        25, 69,  0, 28, 65, 50, 25, 42, 33, 26, 49, 21, 52, 35, 49, 21, 25, 50,\n",
            "        25, 50, 33, 24, 50, 25, 53,  1, 45, 49, 25,  3, 35, 56,  7, 62,  3, 35,\n",
            "        61, 43, 13, 65, 34, 50, 39, 42, 52, 26, 58,  3, 66, 50, 60, 34, 10, 25,\n",
            "        69,  0, 32, 57, 58,  4, 50, 60,  4, 20, 58, 27, 64, 25,  3, 51, 59, 30,\n",
            "         7, 26, 56, 35, 53, 40, 35, 53,  1, 42, 48, 30, 35, 54, 26, 30, 35, 66,\n",
            "        45, 33, 62, 30, 35, 65, 31, 66, 50, 27, 35, 48, 10, 50, 10, 53,  0, 10,\n",
            "        50, 39, 26, 56, 35, 53, 43, 35, 35, 41, 50, 42, 22, 50, 39, 35,  1, 33,\n",
            "        53, 58, 45,  3, 45,  7,  5, 69, 25,  7, 37, 49,  3, 41, 19, 69, 45, 49,\n",
            "         5, 35, 35, 50, 10,  0, 30, 35, 48, 25, 50,  7, 25, 50, 15, 25, 50, 33,\n",
            "        27, 23, 56, 33, 58,  3, 42, 35,  1, 42, 25, 33, 25, 50,  7, 59, 42, 25,\n",
            "        42, 56, 35, 50,  7,  5, 25, 52,  3, 35,  0, 20, 49,  7,  3, 52, 25, 25,\n",
            "        35, 25, 65, 50]))\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data, seq_len):\n",
        "    self.data = data\n",
        "    self.seq_len = seq_len\n",
        "  def __len__(self):\n",
        "    return len(self.data)-seq_len\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx:idx+seq_len], self.data[idx+1:idx+seq_len+1]\n",
        "\n",
        "train_dataset = TextDataset(train_data, seq_len)\n",
        "val_dataset = TextDataset(val_data, seq_len)\n",
        "print(train_dataset[0])\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjjq6aoIV-J4"
      },
      "source": [
        "## Part 1: Self-Attention Head (Scaled Dot-Product Attention)\n",
        "\n",
        "This part implements the 3.2.1 Scaled Dot-Product Attention in the paper _Attention is All You Need_.\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} )V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "X05dtVO7l8VZ"
      },
      "outputs": [],
      "source": [
        "B, T, C = batch_size, seq_len, n_embd # batch, time, channels\n",
        "head_size = n_embd//n_head    # 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtC0yuI0idPM"
      },
      "source": [
        "### 1.1 Implementing Queries, Keys, Values\n",
        "\n",
        "This should be the easiest step of the self-attention. Given $x$ with the shape of $B \\times T \\times C$ (batch size, time/sequence length, channel/text embedding size), multiply it with the Query, Key, and Value embedding matrix to get $q$,$k$,$v$ vectors of shape $B \\times T \\times d_k$. Where $d_k$ is the head size (size of each query, key, value vector).\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "Use `nn.Linear` to define the `key`, `query`, and `value` embedding weights (take note to not include the bias). And calculate the `k`, `q`, and `v` vectors from $x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "HoQqUTxGf-AE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B: 16, T: 256, C: 64, head_size: 16\n",
            "key: torch.Size([16, 64]), query: torch.Size([16, 64]), value: torch.Size([16, 64])\n",
            "tensor([ 0.7020,  0.6227, -0.2063, -1.0227,  0.5020, -1.4468,  0.7205, -0.3498,\n",
            "        -0.8498,  0.9094,  0.1168, -0.9637, -0.4064, -0.0979,  1.5379, -0.0558],\n",
            "       grad_fn=<SelectBackward0>) tensor([-1.0709, -1.1625,  0.0260,  0.3891, -0.5746,  0.1046, -0.5273,  0.1213,\n",
            "         1.1707,  0.2108,  0.4636,  0.3899,  1.4501, -0.0414,  0.9155,  0.0261],\n",
            "       grad_fn=<SelectBackward0>) torch.Size([16, 256, 16]) torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# B, T, C = 1, 1, 16\n",
        "# head_size = 16\n",
        "x = torch.randn(B, T, C)\n",
        "print(f\"B: {B}, T: {T}, C: {C}, head_size: {head_size}\")\n",
        "\n",
        "#### FILL CODE HERE ####\n",
        "# Fill in these weight matrices\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "print(f\"key: {key.weight.shape}, query: {query.weight.shape}, value: {value.weight.shape}\")\n",
        "# key.weight.data = torch.eye(head_size, C)\n",
        "# query.weight.data = torch.eye(head_size, C)\n",
        "# value.weight.data = torch.eye(head_size, C)\n",
        "# print(f\"key: {key.weight.shape}\")\n",
        "\n",
        "# Calculate k,q,v vectors\n",
        "k = key(x)     # (B, T, d_k)\n",
        "q = query(x)   # (B, T, d_k)\n",
        "v = value(x)   # (B, T, d_k)\n",
        "######################\n",
        "print(k[0][0], q[0][0], k.shape, q.shape, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "6UMoK-A6nzIE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16])\n",
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "k_0_0 = torch.Tensor([ 0.0237, -0.3147, -1.2971,  0.2878,  0.0821,  0.9354,  0.0844, -0.3690, -0.3015, -0.3860,  0.4318,  0.0112, -0.2361,  0.2611, -0.1541,  0.3386])\n",
        "q_0_0 = torch.Tensor([0.2891, -0.3608,  0.2564, -0.0138, -0.3222, -0.0433,  0.2870,  0.2117, -0.1908,  0.2134,  0.6257,  0.2312,  0.5987,  1.0243,  0.3936,  0.2903])\n",
        "print(k_0_0.shape)\n",
        "print(torch.allclose(k_0_0, k[0][0].data, atol=1e-4, rtol=0))\n",
        "print(torch.allclose(q_0_0, q[0][0].data, atol=1e-4, rtol=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDb0rYfHhkQ"
      },
      "source": [
        "### Q1: What is the first number of v[0][0]?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjVgWGl4Hj_G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BupSuPVNqwiP"
      },
      "source": [
        "### 1.2 Calculate Dot Product of Query and Value\n",
        "\n",
        "Perform dot product of `q` and `k` using `torch.mm` or `@` such that it has shape $B \\times T \\times T$. Do note that `transpose` is required for this to work, since both are at shape $B \\times T \\times d_k$. And normalize the resulting weights `wei` by $\\sqrt{d_k}$.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/self-attention_softmax.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "Please take a look at the resulting `q` and `k` dot product. In a single batch, a `q` matrix has dimensions $T \\times d_k$ (each row represent the sequence length and columns are the embeddings of head size). We can view each row of `q` as the $\\vec{q}_1$ query vector represented above. The dot product would represent the following resulting matrix:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \\color{red}{q_{1,1}} & \\color{red}{q_{1,2}} & \\color{red}{q_{1,3}} & \\color{red}\\cdots \\\\ q_{2,1} & q_{2,2} & q_{2,3} & \\cdots \\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix} \\color{blue}{k_{1,1}} & \\color{blue}{k_{1,2}} & \\color{blue}{k_{1,3}} & \\color{blue}\\cdots \\\\ k_{2,1} & k_{2,2} & k_{2,3} & \\cdots \\end{bmatrix}^T\n",
        "=\n",
        "\\begin{bmatrix} \\color{red}{\\vec{q}_1} \\cdot \\color{blue}{\\vec{k}_1} & \\color{red}{\\vec{q}_1} \\cdot \\vec{k}_2 \\\\ \\vec{q}_2 \\cdot \\color{blue}{\\vec{k}_1} & \\vec{q}_2 \\cdot \\vec{k}_2  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The resulting matrix would have the dimensions of $T \\times T$, where each row is the attention score of each word. For instance, referencing the image above, the first row is the attention scores of the first word \"Thinking\" compared with the other words in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "9MICq8ZAqwT_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 256, 256])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.7612,  0.2295,  0.0509,  0.2422, -0.0880,  0.7159,  0.3115,  0.1013],\n",
              "        [-0.1423,  0.0154,  0.1419,  0.1402, -0.0295,  0.3767, -0.2689,  0.2378],\n",
              "        [-0.2216, -0.0725, -0.2789, -0.1393, -0.0674,  0.4295,  0.0111, -0.1680],\n",
              "        [-0.0231, -0.1689, -0.3135,  0.2328, -0.0817,  0.0962, -0.0428, -0.5817],\n",
              "        [ 0.1118, -0.1521, -0.2316,  0.1784,  0.1115,  0.0956, -0.1409, -0.0464],\n",
              "        [-0.0497, -0.1134, -0.2892, -0.0801,  0.1453,  0.3804, -0.0347, -0.1634],\n",
              "        [ 0.3202,  0.2832, -0.1353,  0.3179,  0.2472,  0.1691, -0.1433,  0.2831],\n",
              "        [-0.1664, -0.0450, -0.5189, -0.1039,  0.4599, -0.2667,  0.1881,  0.5595]],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#### FILL CODE HERE ####\n",
        "qT = q.transpose(1, 2) # (B, d_k, T)\n",
        "wei =  torch.matmul(k, qT) # (B, T, d_k) @ (B, d_k, T) ---> (B, T, T)\n",
        "wei = wei / (head_size**0.5)\n",
        "\n",
        "print(wei.shape)\n",
        "######################\n",
        "wei[0][:8, :8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_RHqPbOJS2v"
      },
      "source": [
        "### Q2: What shape is `wei` after the dot product of `q` and `k`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "7RUAqf5d8Aqz"
      },
      "outputs": [],
      "source": [
        "# (16, 256, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptQ6-2lfta6g"
      },
      "source": [
        "### 1.3 Perform Masked Attention with `torch.tril`\n",
        "\n",
        "Since we are making an autoregressive decoder-only block, it would be weird for the current token to be able to attend to future tokens. If we look at the figure above, it doesn't make any sense for the word \"Thinking\" to be able to see \"Machines\", else you already know the result to be generated. Hence, you need to \"mask\" these attentions.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "To do this, there is a special kind of matrix called triangular matrix. See the result of `torch.tril` below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "XUechefjblXi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tril(torch.ones(T,T))[:8,:8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybmEPx38-Yrj"
      },
      "source": [
        "Referring to the resulting $Q \\cdot K$ of dimensions $T \\times T$, each row index represents the time dimension, and the columns are all the other words in the sequence. For instance, when we are at the current word \"Thinking\" at time $t=1$, the generation of the second word should not be able to to access $\\vec{q}_1 \\cdot \\vec{k}_1$, and not $\\vec{q}_1 \\cdot \\vec{k}_2$ (corresponding to keys of \"Machines\" and \"are\"). This is illustrated in the matrix below:\n",
        "\n",
        "$$\\require{cancel}$$\n",
        "$$\n",
        "\\begin{matrix} .\\qquad \\text{Thinking} & \\text{Machines} & \\text{are} \\end{matrix} \\\\\n",
        "\\begin{matrix} \\color{blue}{\\text{Thinking}} \\\\ \\text{Machines} \\\\ \\text{are} \\end{matrix}\n",
        "\\begin{bmatrix}  \\color{blue}{\\vec{q}_1 \\cdot \\vec{k}_1} & \\cancel{\\vec{q}_1 \\cdot \\vec{k}_2} & \\cancel{\\vec{q}_1 \\cdot \\vec{k}_3} \\\\ \\vec{q}_2 \\cdot \\vec{k}_1 & \\vec{q}_2 \\cdot \\vec{k}_2 & \\cancel{\\vec{q}_2 \\cdot \\vec{k}_3} \\\\ \\vec{q}_3 \\cdot \\vec{k}_1 & \\vec{q}_3 \\cdot \\vec{k}_2 & \\vec{q}_3 \\cdot \\vec{k}_3 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This is very similar to the triangular matrix. If we are able to filter out the attentions where `tril == 0`, masked self attention will be achieved.\n",
        "\n",
        "Use `masked_fill` on `wei` such that the resulting attention softmax is `0` just like the matrix above.  \n",
        "Note: $-\\infty$ or `float('-inf')` may be required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "mGv2qPjMnrA7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4607, 0.5393, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3221, 0.3738, 0.3041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2562, 0.2214, 0.1916, 0.3308, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2200, 0.1689, 0.1560, 0.2351, 0.2199, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1551, 0.1455, 0.1221, 0.1504, 0.1885, 0.2384, 0.0000, 0.0000],\n",
              "        [0.1663, 0.1602, 0.1054, 0.1659, 0.1546, 0.1430, 0.1046, 0.0000],\n",
              "        [0.0984, 0.1111, 0.0692, 0.1047, 0.1840, 0.0890, 0.1403, 0.2033]],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "#### FILL CODE HERE ####\n",
        "wei = torch.masked_fill(wei, tril == 0, float('-inf'))\n",
        "######################\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "wei[0][:8, :8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Po5K7HhJI_az"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True])"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0][0][1:] == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYHkY6_RKPmL"
      },
      "source": [
        "### 1.4 Calculate Dot Product of Softmax and Value\n",
        "\n",
        "Should be self-explanatory. We are at the final stage of the equation to get a resulting matrix of shape $B \\times T \\times d_k$ (we use the same dimensions for key and value).\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} )V\n",
        "$$\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"500\" />\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "A0ouoYAiI2bi"
      },
      "outputs": [],
      "source": [
        "#### FILL CODE HERE ####\n",
        "out = torch.matmul(wei, v) # (B, T, T) @ (B, T, d_k) ---> (B, T, d_k)\n",
        "######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76P5Pu2_Mo0k"
      },
      "source": [
        "### Q3: What shape is the resulting attention?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "S9QGn_ZaK6PP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 256, 16])"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zhvJj79LiNi"
      },
      "source": [
        "### 1.5 Putting it all together!\n",
        "\n",
        "Now it's time to code the Attention Head `nn.Module`.\n",
        "1. Initialize all the `nn.Linear` in the constructor. Use the hyperparameter `n_embd` for the embedding size.\n",
        "2. Perform the self-attention calculations in the `forward` function\n",
        "\n",
        "Note that there may be some differences in the implemented version:\n",
        "- Since `tril` is not a parameter in the PyTorch module, it is registered as a `buffer` instead.\n",
        "- A `dropout` is appended after the softmax for regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "wkpyS4Ff9qBN"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        #### FILL CODE HERE ####\n",
        "        self.key = nn.Linear(C, head_size, bias=False)\n",
        "        self.query = nn.Linear(C, head_size, bias=False)\n",
        "        self.value = nn.Linear(C, head_size, bias=False)\n",
        "        ######################\n",
        "        self.head_size = head_size\n",
        "\n",
        "        # print(f\"key: {self.key.weight.shape}, query: {self.query.weight.shape}, value: {self.value.weight.shape}\")\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        B, T, C = x.shape\n",
        "        tril = self.tril[:T, :T] == 0\n",
        "        #### FILL CODE HERE ####\n",
        "        k: Tensor = self.key(x)                   # (B, T, d_k)\n",
        "        q: Tensor = self.query(x)                 # (B, T, d_k)\n",
        "        v: Tensor = self.value(x)                 # (B, T, d_k)\n",
        "\n",
        "        # Calculate the attention scores\n",
        "        qT = q.transpose(1, 2) # (B, d_k, T)\n",
        "        wei = torch.matmul(k, qT) * (self.head_size**-0.5)               # Dot product of q * k & normalization (B, T, d_k) @ (B, d_k, T) -> (B, T, T)\n",
        "        wei = torch.masked_fill(wei, tril == 0, float('-inf'))          # Use masked_fill on tril (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)                                    # Apply softmax (B, T, T)\n",
        "        wei = self.dropout(wei)                                         # Added dropout\n",
        "        out = torch.matmul(wei, v)                                      # (B, T, T) @ (B, T, d_k) -> (B, T, d_k)\n",
        "        ######################\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj4hooukM8Sc"
      },
      "source": [
        "### Q4: What is Head's output shape?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "bpXKLTGiLMQd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 256, 16])"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(B,T,C)\n",
        "head = Head(head_size)\n",
        "#### FILL CODE HERE ####\n",
        "out = head(x) \n",
        "out.shape\n",
        "######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILFCwZEkOLof"
      },
      "source": [
        "## Part 2: Multi-Head Attention\n",
        "\n",
        "This part implements the 3.2.2 Multi-Head Attention in the paper _Attention is All You Need`.\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ... ,\\text{head}_h)W^O\\\\\n",
        "\\text{where}\\: \\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzzokhk_C1Qz"
      },
      "source": [
        "With multiple heads running in parallel, this would give rise to multiple representation subspaces, where each head would have multiple sets of Q/K/V matrices.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "The resulting attention would be all concatenated to a long matrix.\n",
        "\n",
        "To preserve the shape of the vector back to the embedding size $C$ before the feed-forward layer (if the concatenation does not have the same size as the embedding), we use a projection layer $W^O$ with dimensions $hd_k \\times C$.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\" width=\"500\" />\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "There are two things you need to implement:\n",
        "- The `self.heads` which is an `nn.ModuleList` of all the Attention `Head` of $h$ (`num_heads`) layers (these will be computed in parallel). And `torch.cat` to concatenante the heads in `forward`.\n",
        "- The `self.proj` projection layer $W^O$ (with dimensions $C \\times C$ as noted below). Use the hyperparameter `n_embd` for the embedding size. Apply the projection accordingly in `forward`.\n",
        "\n",
        "Note that in the paper, they use $d_k = C/h = 64$. The head size is equal to the embedding size divided by the number of heads. So the higher number of heads, the lower the dimension of the head to preserve computational cost equivalent to single head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "XVVLYJs_OR5U"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module): # checked\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        #### FILL CODE HERE ####\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        ######################\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #### FILL CODE HERE ####\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1) # (B, T, d_k) * num_heads -> (B, T, d_k*num_heads)\n",
        "        out = self.proj(out)                                      # (B, T, d_k*num_heads) -> (B, T, C)\n",
        "        ######################\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpIrUDkZN8lL"
      },
      "source": [
        "### Q5: What is MultiHead's output shape?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "JKMC83IJVHZk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 256, 64])"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(B,T,C) # C = 64\n",
        "heads = MultiHeadAttention(n_head, head_size) # 4 heads, each of size 16\n",
        "#### FILL CODE HERE ####\n",
        "out = heads(x)\n",
        "out.shape\n",
        "######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21lJ8cn8P_Vm"
      },
      "source": [
        "## Part 3: Feed Forward\n",
        "\n",
        "At each block, there is a fully connected feed-forward network. Implement the 3.3 Position-wise Feed-Forward Networks in the paper _Attention is All You Need_.\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, xW_1+b_1)W_2 + b_2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5JbegqUUJuv"
      },
      "source": [
        "This part gets the resulting matrix of shape $B \\times T \\times C$ from the Multi-Head Attention. The paper noted that they used an embedding size dimensionality of $C=512$ and the feed-forward inner-layer dimensionality $d_{ff}=2048$, which is pretty much $4C$.\n",
        "\n",
        "Implement the Feed-forward equation up top with `nn.Linear` and `nn.ReLU` with the correct embedding size. Use `n_embd` for embedding size. Preserve the shape of $B \\times T \\times C$ in the resulting matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "MQzzhaynP-vW"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module): # checked\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            #### FILL CODE HERE ####\n",
        "            nn.Linear(n_embd, n_embd*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd*4, n_embd),\n",
        "            ######################\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "X-HQV2C-VDBq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 256, 64])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(B,T,C)\n",
        "_module = FeedFoward(n_embd) # C = n_embd\n",
        "out = _module(x)\n",
        "print(out.shape)\n",
        "out.shape == torch.Size([B,T,n_embd])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAC-UdkXO54K"
      },
      "source": [
        "### Q6: How many parameters are in a FeedForward module?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "V1rW9x2UO7g2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33088"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(p.numel() for p in _module.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqz0rrqQRF3"
      },
      "source": [
        "## Part 4: Transformer Block\n",
        "\n",
        "Putting all the blocks together! We have initialize the constructor with all the defined previous components along with `nn.LayerNorm`.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" width=\"400\" />\n",
        "</div>\n",
        "**Note: DO NOT reference this image**\n",
        "\n",
        "Now I would like you to implement the residual connections, self-attention, feed forward, and the layer norm in `forward`. As a slight deviation from the paper, now it is more common to do pre-norm, which is to apply `LayerNorm` before self-attention.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QnTkcVlyoseiZk65-IHbQhESpwmX6Zfx\" width=\"400\" />\n",
        "</div>\n",
        "\n",
        "1. Apply the first layer norm to $x$, put it through self-attention layer, and add in the residual connection.\n",
        "2. Apply the second layer norm, put it through feed forward layer, and add in the residual connection.\n",
        "\n",
        "Reference: [Andrej Kaparty - Let's build GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY).   \n",
        "[Prenorm](https://arxiv.org/pdf/2002.04745)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "IJdorNIAQRfA"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module): # checked\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #### FILL CODE HERE ####\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        ######################\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYaGUls7QhZ5"
      },
      "source": [
        "## Part 5: Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "oql3z2JGQgUC"
      },
      "outputs": [],
      "source": [
        "class TransformerLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # n_embd: 64\n",
        "        self.position_embedding_table = nn.Embedding(seq_len, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # n_layer: 4\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x)    # (B,T,C)\n",
        "        x = self.ln_f(x)      # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -seq_len:]            # crop idx to the last block_size tokens\n",
        "            logits, loss = self(idx_cond)           # get the predictions\n",
        "            logits = logits[:, -1, :]               # focus only on the last time step - becomes (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)       # apply softmax to get probabilities - (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution - (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # append sampled index to the running sequence - (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "TMgeIGxiSqDv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.224839 M parameters\n"
          ]
        }
      ],
      "source": [
        "class TransformerLMModule(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = TransformerLanguageModel()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        xb, yb = batch\n",
        "        # evaluate the loss\n",
        "        logits, loss = self.model(xb, yb)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        xb, yb = val_batch\n",
        "        logits, loss = self.model(xb, yb)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "\n",
        "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        if batch_idx % self.trainer.log_every_n_steps == 0:\n",
        "            now = datetime.now()\n",
        "            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx}/{self.trainer.max_steps} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')\n",
        "\n",
        "L.pytorch.seed_everything(42)\n",
        "m = TransformerLMModule()\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # print the number of parameters in the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J48ANUs4QHOX"
      },
      "source": [
        "### Q7: How many parameters are in your nanoGPT model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "PFCvaRPkjFOd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name  | Type                     | Params | Mode \n",
            "-----------------------------------------------------------\n",
            "0 | model | TransformerLanguageModel | 224 K  | train\n",
            "-----------------------------------------------------------\n",
            "224 K     Trainable params\n",
            "0         Non-trainable params\n",
            "224 K     Total params\n",
            "0.899     Total estimated model params size (MB)\n",
            "142       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/idhibhatpankam/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/Users/idhibhatpankam/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Placeholder storage has not been allocated on MPS device!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[124], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,  logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \\\n\u001b[1;32m      2\u001b[0m                     max_steps\u001b[38;5;241m=\u001b[39mmax_iters,\n\u001b[1;32m      3\u001b[0m                     val_check_interval \u001b[38;5;241m=\u001b[39m eval_interval,\n\u001b[1;32m      4\u001b[0m                     log_every_n_steps \u001b[38;5;241m=\u001b[39meval_interval,\n\u001b[1;32m      5\u001b[0m                     enable_checkpointing \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                     limit_val_batches \u001b[38;5;241m=\u001b[39m eval_iters)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1024\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:144\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:433\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    427\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    432\u001b[0m )\n\u001b[0;32m--> 433\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    326\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[121], line 19\u001b[0m, in \u001b[0;36mTransformerLMModule.validation_step\u001b[0;34m(self, val_batch, batch_idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, val_batch, batch_idx):\n\u001b[1;32m     18\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m val_batch\n\u001b[0;32m---> 19\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[119], line 16\u001b[0m, in \u001b[0;36mTransformerLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# idx and targets are both (B,T) tensor of integers\u001b[39;00m\n\u001b[1;32m     15\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(idx) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)    \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(deterministic=True, accelerator=\"auto\", devices=\"auto\",  logger=False, \\\n",
        "                    max_steps=max_iters,\n",
        "                    val_check_interval = eval_interval,\n",
        "                    log_every_n_steps =eval_interval,\n",
        "                    enable_checkpointing =False,\n",
        "                    limit_val_batches = eval_iters)\n",
        "trainer.fit(m, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UMES7hETe06"
      },
      "source": [
        "### Q8: What's the perplexity (from validation loss) on Step 5000?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G655a_8TipY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LpwBtAOTlDi"
      },
      "source": [
        "### Q9: What's your output from the generated text?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAbQtCkEh4UG"
      },
      "outputs": [],
      "source": [
        "L.pytorch.seed_everything(42)\n",
        "# generate from the model\n",
        "context = torch.tensor([encode(\"๏ อาจารย์พีรพลสอนเอ็นแอลพี\t\")], dtype=torch.long, device=device)\n",
        "print(decode(m.model.to(device).generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5ND0b5nVUvN"
      },
      "source": [
        "## Part 6: Transformer from HuggingFace\n",
        "\n",
        "In this part you will be using `transformers` from HuggingFace, the go-to library for many models. We will be, in similar fashion, training DistilGPT2 on the same dataset. I have provided you the tokenizer and Dataloader for ease of use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqoVrwdmVUBe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "358YrYXlV9Q2"
      },
      "outputs": [],
      "source": [
        "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
        "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
        "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
        "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohb0NHQOWB22"
      },
      "outputs": [],
      "source": [
        "seq_len=768\n",
        "batch_size=8\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(text)) # first 90% will be train, rest val\n",
        "train_text = text[:n]\n",
        "val_text = text[n:]\n",
        "\n",
        "class GPT2TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, text, tokenizer, seq_len=768):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "    for i in range(0, len(text)//700):\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ text[i*700:(i+1)*700] + '<|endoftext|>', truncation=True, max_length=seq_len, padding=\"max_length\")\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "train_dataset = GPT2TextDataset(train_text, tokenizer, seq_len)\n",
        "val_dataset = GPT2TextDataset(val_text, tokenizer, seq_len)\n",
        "print(len(train_dataset), len(val_dataset))\n",
        "print(train_dataset[0][0].shape)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfz2LJijWY9L"
      },
      "source": [
        "Read how Auto Classes work in HuggingFace https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes , and use `AutoModelForCausalLM.from_pretrained` to initialize your `distilgpt2` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "pPWfz-UaWEOO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/idhibhatpankam/Code/courses/NLP-SYS/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Seed set to 42\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'resize_token_embeddings'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[123], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, tokenizer\u001b[38;5;241m.\u001b[39mdecode(sample_output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrip()))\n\u001b[1;32m     42\u001b[0m L\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mseed_everything(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m distilgpt2 \u001b[38;5;241m=\u001b[39m \u001b[43mDistilGPT2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m distilgpt2\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM parameters\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# print the number of parameters in the model\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[123], line 9\u001b[0m, in \u001b[0;36mDistilGPT2.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m######################\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize_token_embeddings\u001b[49m(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'resize_token_embeddings'"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "class DistilGPT2(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #### FILL CODE HERE ####\n",
        "        self.model = None\n",
        "        ######################\n",
        "        self.model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        xb, mask = batch\n",
        "        # evaluate the loss\n",
        "        loss, logits = self.model(xb, labels=xb, attention_mask=mask, token_type_ids=None)[:2]\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        xb, mask = val_batch\n",
        "        loss, logits = self.model(xb, labels=xb, attention_mask=mask, token_type_ids=None)[:2]\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "\n",
        "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        if batch_idx % self.trainer.log_every_n_steps == 0:\n",
        "            now = datetime.now()\n",
        "            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        beginning_text = tokenizer('<|startoftext|>'+\"แต่ปางหลังยังมีกรุงกษัตริย์\", return_tensors=\"pt\").to(device)\n",
        "        sample_outputs = self.model.generate(beginning_text['input_ids'], attention_mask=beginning_text[\"attention_mask\"],pad_token_id=tokenizer.pad_token_id, do_sample=True, max_length =300, num_return_sequences=1)\n",
        "        for i, sample_output in enumerate(sample_outputs):\n",
        "              print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True).strip()))\n",
        "\n",
        "L.pytorch.seed_everything(42)\n",
        "distilgpt2 = DistilGPT2()\n",
        "print(sum(p.numel() for p in distilgpt2.parameters())/1e6, 'M parameters') # print the number of parameters in the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTf-fav6XIne"
      },
      "source": [
        "### Q10: How many parameters are in the DistilGPT2 model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7UG1UIScyjD"
      },
      "source": [
        "<font color='red'>Training should take around ~18 minutes</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "btFuxXQWXIXw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'distilgpt2' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[122], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,  logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \\\n\u001b[1;32m      2\u001b[0m                     max_epochs \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      3\u001b[0m                     log_every_n_steps \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader),\n\u001b[1;32m      4\u001b[0m                     enable_checkpointing \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                     )\n\u001b[0;32m----> 6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdistilgpt2\u001b[49m, train_dataloader, val_dataloader)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'distilgpt2' is not defined"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(deterministic=True, accelerator=\"auto\", devices=\"auto\",  logger=False, \\\n",
        "                    max_epochs =5,\n",
        "                    log_every_n_steps =len(train_dataloader),\n",
        "                    enable_checkpointing =False,\n",
        "                    )\n",
        "trainer.fit(distilgpt2, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcy3dcGc2RM"
      },
      "source": [
        "### Q11: What's the perplexity (from validation loss) on the last step?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnHTZjgJc5yo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUcvTGYMc6ee"
      },
      "source": [
        "### Q12: What's the output from the generated text?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh6UNBw6c8TO"
      },
      "outputs": [],
      "source": [
        "L.pytorch.seed_everything(40)\n",
        "beginning_text = tokenizer('<|startoftext|>'+\"อาจารย์พีรพลสอนเอ็นแอลพี\", return_tensors=\"pt\")\n",
        "output = distilgpt2.model.generate(beginning_text['input_ids'], attention_mask=beginning_text[\"attention_mask\"],pad_token_id=tokenizer.pad_token_id, do_sample=True, max_length =500, num_return_sequences=1)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True).strip())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
